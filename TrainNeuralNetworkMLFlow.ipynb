{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io, os, time\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_dataset(filename='robot_arm_dataset_10M.npz'):\n",
    "    data = np.load(f'./Data/{filename}')\n",
    "    return data['inputs'], data['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_model(input_shape, output_shape):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        keras.layers.Dense(128),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(64),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(output_shape)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseLoggingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = io.StringIO()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        output = f\"Epoch {epoch+1}/{self.params['epochs']} - \"\n",
    "        output += \" - \".join(f\"{k}: {v:.4f}\" for k, v in logs.items())\n",
    "        print(output)\n",
    "        self.output.write(output + \"\\n\")\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.output.getvalue()\n",
    "    \n",
    "class LearningRateLogger(keras.callbacks.Callback):\n",
    "    def __init__(self, tensorboard_writer):\n",
    "        super().__init__()\n",
    "        self.tensorboard_writer = tensorboard_writer\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'value'):\n",
    "            lr = lr.value()\n",
    "        with self.tensorboard_writer.as_default():\n",
    "            tf.summary.scalar('learning_rate', data=lr, step=epoch)\n",
    "        mlflow.log_metric(\"learning_rate\", lr, step=epoch)\n",
    "        \n",
    "class CosineDecayWithWarmupCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_learning_rate, warmup_steps, total_steps):\n",
    "        super(CosineDecayWithWarmupCallback, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            lr = self.initial_learning_rate * (self.current_step / self.warmup_steps)\n",
    "        else:\n",
    "            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            lr = 0.5 * self.initial_learning_rate * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        self.current_step += 1\n",
    "        \n",
    "\n",
    "def forward_kinematics_3dof_vectorized(thetas, l1, l2, l3):\n",
    "    theta1, theta2, theta3 = thetas[:, 0], thetas[:, 1], thetas[:, 2]\n",
    "    x = l1 * np.cos(theta1) * np.sin(theta2) + l2 * np.cos(theta1) * np.sin(theta2 + theta3)\n",
    "    y = l1 * np.sin(theta1) * np.sin(theta2) + l2 * np.sin(theta1) * np.sin(theta2 + theta3)\n",
    "    z = l1 * np.cos(theta2) + l2 * np.cos(theta2 + theta3) + l3\n",
    "    return np.column_stack((x, y, z))\n",
    "\n",
    "def evaluate_inverse_kinematics(model, X_test, y_test, l1, l2, l3):\n",
    "    # Predict joint angles\n",
    "    predicted_thetas = model.predict(X_test)\n",
    "    \n",
    "    # Apply forward kinematics to predicted thetas (vectorized)\n",
    "    predicted_positions = forward_kinematics_3dof_vectorized(predicted_thetas, l1, l2, l3)\n",
    "    \n",
    "    # Calculate MSE between predicted positions and actual positions (X_test)\n",
    "    mse = mean_squared_error(X_test, predicted_positions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Calculate average Euclidean distance\n",
    "    euclidean_distances = np.sqrt(np.sum((X_test - predicted_positions)**2, axis=1))\n",
    "    avg_distance = np.mean(euclidean_distances)\n",
    "    \n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"avg_euclidean_distance\": avg_distance\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(batch_size, epochs, initial_learning_rate, test_size=0.2, experiment_name=\"Inverse Kinematics NN\", run_name=None):\n",
    "    # Set up MLflow\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    # Generate a unique run name if one is provided\n",
    "    if run_name:\n",
    "        timestamp = int(time.time())\n",
    "        unique_run_name = f\"{run_name}_{timestamp}\"\n",
    "    else:\n",
    "        unique_run_name = None\n",
    "        \n",
    "    with mlflow.start_run(run_name=unique_run_name) as run:\n",
    "        # Create a consistent directory structure for TensorBoard logs\n",
    "        run_id = run.info.run_id\n",
    "        run_name = run.data.tags.get('mlflow.runName', run_id)\n",
    "        log_dir = os.path.join(\"logs\", experiment_name, unique_run_name)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        \n",
    "        # Load and split the data\n",
    "        X, y = load_dataset()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        # Print the shapes\n",
    "        print(\"X_train shape:\", X_train.shape)\n",
    "        print(\"X_test shape:\", X_test.shape)\n",
    "        print(\"y_train shape:\", y_train.shape)\n",
    "        print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"initial_learning_rate\", initial_learning_rate)\n",
    "        mlflow.log_param(\"test_size\", test_size)\n",
    "\n",
    "        # Calculate total steps\n",
    "        steps_per_epoch = len(X_train) // batch_size\n",
    "        total_steps = steps_per_epoch * epochs\n",
    "        warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n",
    "        \n",
    "        # Create and compile the model\n",
    "        model = create_model(input_shape=(3,), output_shape=3)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Log model summary\n",
    "        model_summary = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: model_summary.write(x + '\\n'))\n",
    "        mlflow.log_text(model_summary.getvalue(), \"model_summary.txt\")\n",
    "\n",
    "        # Set up TensorBoard callback and writer\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        tensorboard_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "        # Set up other callbacks\n",
    "        early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        lr_scheduler = CosineDecayWithWarmupCallback(initial_learning_rate, warmup_steps, total_steps)\n",
    "        lr_logger = LearningRateLogger(tensorboard_writer)\n",
    "        \n",
    "        callbacks = [tensorboard_callback, lr_scheduler, lr_logger]\n",
    "\n",
    "        # Log callback names\n",
    "        callback_names = [callback.__class__.__name__ for callback in callbacks]\n",
    "        mlflow.log_param(\"callbacks\", \", \".join(callback_names))\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Log metrics\n",
    "        for epoch, (loss, val_loss) in enumerate(zip(\n",
    "            history.history['loss'],\n",
    "            history.history['val_loss']\n",
    "        )):\n",
    "            mlflow.log_metric(\"train_loss\", loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "        # Log the TensorBoard log directory\n",
    "        mlflow.log_param(\"tensorboard_log_dir\", log_dir)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        \n",
    "        # After training, evaluate the model\n",
    "        evaluation_results = evaluate_inverse_kinematics(model, X_test, y_test, l1=1.0, l2=1.5, l3=0.5)\n",
    "\n",
    "        # Log the evaluation results\n",
    "        mlflow.log_metric(\"test_mse\", evaluation_results[\"mse\"])\n",
    "        mlflow.log_metric(\"test_rmse\", evaluation_results[\"rmse\"])\n",
    "        mlflow.log_metric(\"test_avg_euclidean_distance\", evaluation_results[\"avg_euclidean_distance\"])\n",
    "\n",
    "    print(\"Training completed and logged with MLflow and TensorBoard.\")\n",
    "    print(f\"Experiment name: {experiment_name}\")\n",
    "    print(f\"Run name: {run_name}\")\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"TensorBoard logs saved to: {log_dir}\")\n",
    "    print(\"To view in TensorBoard, run:\")\n",
    "    print(f\"tensorboard --logdir logs/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = [\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.1,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_baseline\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**14,\n",
    "        \"epochs\": 100,\n",
    "        \"initial_learning_rate\": 0.01,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_smaller_batch_lower_lr\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**16,\n",
    "        \"epochs\": 30,\n",
    "        \"initial_learning_rate\": 0.001,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_larger_batch_lowest_lr\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 75,\n",
    "        \"initial_learning_rate\": 0.05,\n",
    "        \"test_size\": 0.15,\n",
    "        \"run_name\": \"CosineWithWarmUp_medium_lr_smaller_test\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**13,\n",
    "        \"epochs\": 150,\n",
    "        \"initial_learning_rate\": 0.1,\n",
    "        \"test_size\": 0.25,\n",
    "        \"run_name\": \"CosineWithWarmUp_smallest_batch_high_epochs\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**17,\n",
    "        \"epochs\": 25,\n",
    "        \"initial_learning_rate\": 0.2,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_largest_batch_highest_lr\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**14,\n",
    "        \"epochs\": 60,\n",
    "        \"initial_learning_rate\": 0.075,\n",
    "        \"test_size\": 0.18,\n",
    "        \"run_name\": \"CosineWithWarmUp_balanced_approach\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 40,\n",
    "        \"initial_learning_rate\": 0.15,\n",
    "        \"test_size\": 0.22,\n",
    "        \"run_name\": \"CosineWithWarmUp_higher_lr_more_test\"\n",
    "    }\n",
    "]\n",
    "\n",
    "configurations2 = [\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.1,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"SELU_CosineWithWarmUp_baseline_lr0.1\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.3,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"SELU_CosineWithWarmUp_high_lr0.3\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.01,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"SELU_CosineWithWarmUp_low_lr0.01\"\n",
    "    }\n",
    "]\n",
    "\n",
    "configurations3 = [\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.05,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_baseline_lr0.05\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Example usage:\n",
    "for config in configurations3:\n",
    "    train_model(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "  1/245 [..............................] - ETA: 6:08 - loss: 1.5305WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0058s). Check your callbacks.\n",
      "245/245 [==============================] - 4s 9ms/step - loss: 0.3452 - val_loss: 0.3982\n",
      "Epoch 2/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2548 - val_loss: 1.0729\n",
      "Epoch 3/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2427 - val_loss: 0.4258\n",
      "Epoch 4/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2350 - val_loss: 0.2893\n",
      "Epoch 5/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2317 - val_loss: 0.3180\n",
      "Epoch 6/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2296 - val_loss: 0.3013\n",
      "Epoch 7/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2282 - val_loss: 0.2603\n",
      "Epoch 8/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2263 - val_loss: 0.2747\n",
      "Epoch 9/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2241 - val_loss: 0.2438\n",
      "Epoch 10/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2251 - val_loss: 0.2451\n",
      "Epoch 11/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2240 - val_loss: 0.2270\n",
      "Epoch 12/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2234 - val_loss: 0.2344\n",
      "Epoch 13/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2226 - val_loss: 0.2238\n",
      "Epoch 14/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2210 - val_loss: 0.2253\n",
      "Epoch 15/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2203 - val_loss: 0.2337\n",
      "Epoch 16/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2207 - val_loss: 0.2217\n",
      "Epoch 17/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2192 - val_loss: 0.2199\n",
      "Epoch 18/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2185 - val_loss: 0.2208\n",
      "Epoch 19/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2170 - val_loss: 0.2166\n",
      "Epoch 20/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2167 - val_loss: 0.2147\n",
      "Epoch 21/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2160 - val_loss: 0.2159\n",
      "Epoch 22/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2153 - val_loss: 0.2122\n",
      "Epoch 23/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2144 - val_loss: 0.2113\n",
      "Epoch 24/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2142 - val_loss: 0.2105\n",
      "Epoch 25/25\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2142 - val_loss: 0.2103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 20:06:24 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8_ce1jw0/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2024/08/16 20:06:30 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62500/62500 [==============================] - 156s 2ms/step\n",
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN_IKEval\n",
      "Run name: CosineWithWarmUp_e25_lr0.1_1723838729\n",
      "Run ID: 1599f6498d2449a78c34403cdb5a0cba\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN_IKEval/CosineWithWarmUp_e25_lr0.1_1723838729\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN_IKEval\n",
      "Epoch 1/25\n",
      "  1/245 [..............................] - ETA: 3:27 - loss: 1.5995WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0058s). Check your callbacks.\n",
      "245/245 [==============================] - 3s 9ms/step - loss: 0.3498 - val_loss: 0.4107\n",
      "Epoch 2/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2542 - val_loss: 0.4716\n",
      "Epoch 3/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2410 - val_loss: 0.3967\n",
      "Epoch 4/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2371 - val_loss: 0.3116\n",
      "Epoch 5/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2330 - val_loss: 0.2925\n",
      "Epoch 6/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2310 - val_loss: 0.2811\n",
      "Epoch 7/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2306 - val_loss: 0.2542\n",
      "Epoch 8/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2294 - val_loss: 0.2667\n",
      "Epoch 9/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2276 - val_loss: 0.2431\n",
      "Epoch 10/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2261 - val_loss: 0.2724\n",
      "Epoch 11/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2263 - val_loss: 0.2489\n",
      "Epoch 12/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2253 - val_loss: 0.2387\n",
      "Epoch 13/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2236 - val_loss: 0.2243\n",
      "Epoch 14/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2232 - val_loss: 0.2293\n",
      "Epoch 15/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2218 - val_loss: 0.2253\n",
      "Epoch 16/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2209 - val_loss: 0.2213\n",
      "Epoch 17/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2200 - val_loss: 0.2245\n",
      "Epoch 18/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2192 - val_loss: 0.2159\n",
      "Epoch 19/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2187 - val_loss: 0.2164\n",
      "Epoch 20/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2178 - val_loss: 0.2166\n",
      "Epoch 21/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2163 - val_loss: 0.2143\n",
      "Epoch 22/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2154 - val_loss: 0.2139\n",
      "Epoch 23/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2150 - val_loss: 0.2141\n",
      "Epoch 24/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2142 - val_loss: 0.2107\n",
      "Epoch 25/25\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2142 - val_loss: 0.2105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 20:10:34 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8y97hf92/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 20:10:39 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62500/62500 [==============================] - 154s 2ms/step\n",
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN_IKEval\n",
      "Run name: CosineWithWarmUp_e25_lr0.15_1723838980\n",
      "Run ID: 8e73510f57fa408b85756fac094b5723\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN_IKEval/CosineWithWarmUp_e25_lr0.15_1723838980\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN_IKEval\n"
     ]
    }
   ],
   "source": [
    "configurations = [\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 25,\n",
    "        \"initial_learning_rate\": 0.1,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_e25_lr0.1\",\n",
    "        \"experiment_name\": \"Inverse Kinematics NN_IKEval\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 25,\n",
    "        \"initial_learning_rate\": 0.15,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_e25_lr0.15\",\n",
    "        \"experiment_name\": \"Inverse Kinematics NN_IKEval\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Example usage:\n",
    "for config in configurations:\n",
    "    train_model(**config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
