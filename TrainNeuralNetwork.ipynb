{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io, os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_dataset(filename='robot_arm_dataset_10M.npz'):\n",
    "    data = np.load(f'./Data/{filename}')\n",
    "    return data['inputs'], data['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_model(input_shape, output_shape):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        keras.layers.Dense(128),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(64),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(output_shape)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseLoggingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = io.StringIO()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        output = f\"Epoch {epoch+1}/{self.params['epochs']} - \"\n",
    "        output += \" - \".join(f\"{k}: {v:.4f}\" for k, v in logs.items())\n",
    "        print(output)\n",
    "        self.output.write(output + \"\\n\")\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.output.getvalue()\n",
    "    \n",
    "class LearningRateLogger(keras.callbacks.Callback):\n",
    "    def __init__(self, tensorboard_writer):\n",
    "        super().__init__()\n",
    "        self.tensorboard_writer = tensorboard_writer\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'value'):\n",
    "            lr = lr.value()\n",
    "        with self.tensorboard_writer.as_default():\n",
    "            tf.summary.scalar('learning_rate', data=lr, step=epoch)\n",
    "        mlflow.log_metric(\"learning_rate\", lr, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(batch_size, epochs, learning_rate, test_size=0.2, experiment_name=\"Inverse Kinematics NN\", run_name=None):\n",
    "    # Set up MLflow\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    # Generate a unique run name if one is provided\n",
    "    if run_name:\n",
    "        timestamp = int(time.time())\n",
    "        unique_run_name = f\"{run_name}_{timestamp}\"\n",
    "    else:\n",
    "        unique_run_name = None\n",
    "        \n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # Create a consistent directory structure for TensorBoard logs\n",
    "        run_id = run.info.run_id\n",
    "        run_name = run.data.tags.get('mlflow.runName', run_id)\n",
    "        log_dir = os.path.join(\"logs\", experiment_name, f\"{run_name}_{run_id}\")\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "        # Load and split the data\n",
    "        X, y = load_dataset()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"initial_learning_rate\", learning_rate)\n",
    "        mlflow.log_param(\"test_size\", test_size)\n",
    "\n",
    "        # Create and compile the model\n",
    "        model = create_model(input_shape=(3,), output_shape=3)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Log model summary\n",
    "        model_summary = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: model_summary.write(x + '\\n'))\n",
    "        mlflow.log_text(model_summary.getvalue(), \"model_summary.txt\")\n",
    "\n",
    "        # Set up TensorBoard callback and writer\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        tensorboard_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "        # Set up other callbacks\n",
    "        early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=3)\n",
    "        lr_logger = LearningRateLogger(tensorboard_writer)\n",
    "        \n",
    "        callbacks = [tensorboard_callback, early_stopping, reduce_lr, lr_logger]\n",
    "\n",
    "        # Log callback names\n",
    "        callback_names = [callback.__class__.__name__ for callback in callbacks]\n",
    "        mlflow.log_param(\"callbacks\", \", \".join(callback_names))\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Log metrics\n",
    "        for epoch, (loss, val_loss) in enumerate(zip(\n",
    "            history.history['loss'],\n",
    "            history.history['val_loss']\n",
    "        )):\n",
    "            mlflow.log_metric(\"train_loss\", loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "        # Log the TensorBoard log directory\n",
    "        mlflow.log_param(\"tensorboard_log_dir\", log_dir)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "\n",
    "    print(\"Training completed and logged with MLflow and TensorBoard.\")\n",
    "    print(f\"Experiment name: {experiment_name}\")\n",
    "    print(f\"Run name: {run_name}\")\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"TensorBoard logs saved to: {log_dir}\")\n",
    "    print(\"To view in TensorBoard, run:\")\n",
    "    print(f\"tensorboard --logdir logs/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 18:03:41 INFO mlflow.tracking.fluent: Experiment with name 'Inverse Kinematics NN' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/245 [..............................] - ETA: 6:01 - loss: 1.6936WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0045s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.\n",
      "245/245 [==============================] - 4s 8ms/step - loss: 0.3628 - val_loss: 0.5954 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2553 - val_loss: 0.3657 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2438 - val_loss: 0.2637 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2392 - val_loss: 0.2457 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2362 - val_loss: 0.2409 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2342 - val_loss: 0.2393 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2331 - val_loss: 0.2376 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2318 - val_loss: 0.2360 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2304 - val_loss: 0.2335 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2300 - val_loss: 0.2322 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2292 - val_loss: 0.2420 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2283 - val_loss: 0.2344 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2282 - val_loss: 0.2289 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2280 - val_loss: 0.2340 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2277 - val_loss: 0.2319 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2264 - val_loss: 0.2335 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2243 - val_loss: 0.2227 - lr: 2.0000e-04\n",
      "Epoch 18/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2239 - val_loss: 0.2232 - lr: 2.0000e-04\n",
      "Epoch 19/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2239 - val_loss: 0.2226 - lr: 2.0000e-04\n",
      "Epoch 20/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2238 - val_loss: 0.2224 - lr: 2.0000e-04\n",
      "Epoch 21/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2236 - val_loss: 0.2223 - lr: 2.0000e-04\n",
      "Epoch 22/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2240 - val_loss: 0.2217 - lr: 2.0000e-04\n",
      "Epoch 23/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2234 - val_loss: 0.2226 - lr: 2.0000e-04\n",
      "Epoch 24/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2238 - val_loss: 0.2223 - lr: 2.0000e-04\n",
      "Epoch 25/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2235 - val_loss: 0.2226 - lr: 2.0000e-04\n",
      "Epoch 26/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2226 - val_loss: 0.2204 - lr: 4.0000e-05\n",
      "Epoch 27/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2225 - val_loss: 0.2204 - lr: 4.0000e-05\n",
      "Epoch 28/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2225 - val_loss: 0.2206 - lr: 4.0000e-05\n",
      "Epoch 29/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2225 - val_loss: 0.2203 - lr: 4.0000e-05\n",
      "Epoch 30/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2225 - val_loss: 0.2201 - lr: 8.0000e-06\n",
      "Epoch 31/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2224 - val_loss: 0.2201 - lr: 8.0000e-06\n",
      "Epoch 32/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2226 - val_loss: 0.2201 - lr: 8.0000e-06\n",
      "Epoch 33/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2222 - val_loss: 0.2201 - lr: 8.0000e-06\n",
      "Epoch 34/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2224 - val_loss: 0.2200 - lr: 1.6000e-06\n",
      "Epoch 35/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2226 - val_loss: 0.2200 - lr: 1.6000e-06\n",
      "Epoch 36/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2224 - val_loss: 0.2200 - lr: 1.6000e-06\n",
      "Epoch 37/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2224 - val_loss: 0.2200 - lr: 3.2000e-07\n",
      "Epoch 38/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2224 - val_loss: 0.2200 - lr: 3.2000e-07\n",
      "Epoch 39/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2225 - val_loss: 0.2200 - lr: 3.2000e-07\n",
      "Epoch 40/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2225 - val_loss: 0.2200 - lr: 6.4000e-08\n",
      "Epoch 41/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2227 - val_loss: 0.2200 - lr: 6.4000e-08\n",
      "Epoch 42/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2224 - val_loss: 0.2200 - lr: 6.4000e-08\n",
      "Epoch 43/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2223 - val_loss: 0.2200 - lr: 1.2800e-08\n",
      "Epoch 44/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2223 - val_loss: 0.2200 - lr: 1.2800e-08\n",
      "Epoch 45/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2225 - val_loss: 0.2200 - lr: 1.2800e-08\n",
      "Epoch 46/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2224 - val_loss: 0.2200 - lr: 2.5600e-09\n",
      "Epoch 47/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2223 - val_loss: 0.2200 - lr: 2.5600e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 18:05:10 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4uxsy7ik/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2024/08/16 18:05:15 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN\n",
      "Run name: SimpleLRScheduler\n",
      "Run ID: 4019ecd4c16545dc8e853a45d7c1a0ed\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN/SimpleLRScheduler_4019ecd4c16545dc8e853a45d7c1a0ed\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    batch_size=2**15,\n",
    "    epochs=50,  # Increased epochs to demonstrate early stopping\n",
    "    learning_rate=0.001,\n",
    "    test_size=0.2,\n",
    "    run_name=\"SimpleLRScheduler\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
