{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import os\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "L1, L2, L3 = 1.0, 1.5, 0.5  # link lengths\n",
    "\n",
    "@tf.function\n",
    "def forward_kinematics_tf(theta):\n",
    "    theta1, theta2, theta3 = tf.unstack(theta, axis=1)\n",
    "    \n",
    "    x = L1 * tf.cos(theta1) * tf.sin(theta2) + L2 * tf.cos(theta1) * tf.sin(theta2 + theta3)\n",
    "    y = L1 * tf.sin(theta1) * tf.sin(theta2) + L2 * tf.sin(theta1) * tf.sin(theta2 + theta3)\n",
    "    z = L1 * tf.cos(theta2) + L2 * tf.cos(theta2 + theta3) + L3\n",
    "    \n",
    "    return tf.stack([x, y, z], axis=1)\n",
    "\n",
    "def evaluate_model(model, test_inputs, test_outputs, input_mean, input_std, batch_size=2**16):\n",
    "    test_inputs = tf.convert_to_tensor(test_inputs, dtype=tf.float32)\n",
    "    input_mean = tf.convert_to_tensor(input_mean, dtype=tf.float32)\n",
    "    input_std = tf.convert_to_tensor(input_std, dtype=tf.float32)\n",
    "    predicted_angles_normalized = model.predict(test_inputs, batch_size=batch_size)\n",
    "    predicted_angles = predicted_angles_normalized * (np.pi/2)\n",
    "    true_xyz = test_inputs * input_std + input_mean\n",
    "    predicted_xyz = forward_kinematics_tf(predicted_angles)\n",
    "    errors = tf.norm(true_xyz - predicted_xyz, axis=1)\n",
    "    return errors.numpy(), true_xyz.numpy(), predicted_xyz.numpy()\n",
    "\n",
    "def custom_loss(fk_weight=1, delta=0.1):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Huber loss for joint angles\n",
    "        angle_loss = huber_loss(y_true, y_pred, delta)\n",
    "        \n",
    "        # Forward kinematics loss (using Huber loss)\n",
    "        fk_true = forward_kinematics_tf(y_true)\n",
    "        fk_pred = forward_kinematics_tf(y_pred)\n",
    "        fk_loss = huber_loss(fk_true, fk_pred, delta)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = tf.reduce_mean(angle_loss) + fk_weight * tf.reduce_mean(fk_loss)\n",
    "        return total_loss\n",
    "    return loss_fn\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) <= delta\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = delta * (tf.abs(error) - delta / 2)\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "def huber_exp_loss(y_true, y_pred, delta=0.2, alpha=1):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) <= delta\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    exp_loss = alpha * (tf.exp(tf.abs(error) - delta) - 1) + delta * tf.abs(error) - delta**2 / 2\n",
    "    return tf.where(is_small_error, squared_loss, exp_loss)\n",
    "\n",
    "def log_cosh_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.math.log(tf.math.cosh(y_pred - y_true)))\n",
    "\n",
    "def create_model(config):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, activation='relu', input_shape=(3,)),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(3, activation='tanh')\n",
    "    ])\n",
    "    \n",
    "    loss_type = config['loss_type']\n",
    "    if loss_type == 'standard':\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "    elif loss_type == 'custom':\n",
    "        model.compile(optimizer='adam', loss=custom_loss())\n",
    "    elif loss_type == 'modified_custom':\n",
    "        model.compile(optimizer='adam', loss=custom_loss(fk_weight=config.get('fk_weight', 10)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_and_preprocess_data(filename='robot_arm_dataset_10M.npz'):\n",
    "    data = np.load(f'./Data/{filename}')\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    \n",
    "    input_mean = np.mean(inputs, axis=0)\n",
    "    input_std = np.std(inputs, axis=0)\n",
    "    inputs_normalized = (inputs - input_mean) / input_std\n",
    "\n",
    "    outputs_normalized = outputs / (np.pi/2)\n",
    "\n",
    "    split_index = int(0.9 * len(inputs))\n",
    "    train_inputs, test_inputs = inputs_normalized[:split_index], inputs_normalized[split_index:]\n",
    "    train_outputs, test_outputs = outputs_normalized[:split_index], outputs_normalized[split_index:]\n",
    "\n",
    "    return (train_inputs, train_outputs), (test_inputs, test_outputs), input_mean, input_std\n",
    "\n",
    "class VerboseLoggingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = io.StringIO()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        output = f\"Epoch {epoch+1}/{self.params['epochs']} - \"\n",
    "        output += \" - \".join(f\"{k}: {v:.4f}\" for k, v in logs.items())\n",
    "        print(output)\n",
    "        self.output.write(output + \"\\n\")\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.output.getvalue()\n",
    "\n",
    "class LearningRateLogger(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'value'):\n",
    "            lr = lr.value()\n",
    "        mlflow.log_metric(\"learning_rate\", lr, step=epoch)\n",
    "        \n",
    "class CosineDecayWithWarmupCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_learning_rate, warmup_steps, total_steps):\n",
    "        super(CosineDecayWithWarmupCallback, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            lr = self.initial_learning_rate * (self.current_step / self.warmup_steps)\n",
    "        else:\n",
    "            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            lr = 0.5 * self.initial_learning_rate * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        self.current_step += 1\n",
    "\n",
    "def plot_error_distribution(errors, title, save_path=None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(errors, bins=400, alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Error (Euclidean distance)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlim([0, 0.5])\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_true_vs_predicted(true_xyz, predicted_xyz, title, save_path=None):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    for i, coord in enumerate(['X', 'Y', 'Z']):\n",
    "        ax = axes[i]\n",
    "        ax.scatter(true_xyz[:, i], predicted_xyz[:, i], alpha=0.1)\n",
    "        ax.plot([true_xyz[:, i].min(), true_xyz[:, i].max()], [true_xyz[:, i].min(), true_xyz[:, i].max()], 'r--')\n",
    "        ax.set_xlabel(f'True {coord}')\n",
    "        ax.set_ylabel(f'Predicted {coord}')\n",
    "        ax.set_title(f'{coord} Coordinate: True vs Predicted')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "def train_and_evaluate_model(train_data, valid_data, test_data, input_mean, input_std, config):\n",
    "    train_inputs, train_outputs = train_data\n",
    "    valid_inputs, valid_outputs = valid_data\n",
    "    test_inputs, test_outputs = test_data\n",
    "    \n",
    "    experiment_name = config['experiment_name']\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=config['model_name']):\n",
    "        # Calculate total steps\n",
    "        steps_per_epoch = len(train_inputs) // config['batch_size']\n",
    "        total_steps = steps_per_epoch * epochs\n",
    "        warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(config)\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = create_model(config)\n",
    "        \n",
    "        # Log model summary\n",
    "        model_summary = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: model_summary.write(x + '\\n'))\n",
    "        mlflow.log_text(model_summary.getvalue(), \"model_summary.txt\")\n",
    "        \n",
    "        # Set up callbacks\n",
    "        early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        \n",
    "        lr_scheduler = CosineDecayWithWarmupCallback(\n",
    "            config['initial_learning_rate'],\n",
    "            warmup_steps,\n",
    "            total_steps\n",
    "        )\n",
    "        \n",
    "        verbose_logging = VerboseLoggingCallback()\n",
    "        lr_logger = LearningRateLogger()\n",
    "        \n",
    "        callbacks = [early_stopping, lr_scheduler, verbose_logging, lr_logger]\n",
    "        \n",
    "        # Log callback names\n",
    "        callback_names = [callback.__class__.__name__ for callback in callbacks]\n",
    "        mlflow.log_param(\"callbacks\", \", \".join(callback_names))\n",
    "        \n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            train_inputs, train_outputs,\n",
    "            epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            validation_data=(valid_inputs, valid_outputs),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Log training metrics\n",
    "        for epoch, (loss, val_loss) in enumerate(zip(history.history['loss'], history.history['val_loss'])):\n",
    "            mlflow.log_metric(\"train_loss\", loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        \n",
    "        mlflow.log_metric(\"training_time\", training_time)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        \n",
    "        # Log training output\n",
    "        mlflow.log_text(verbose_logging.get_output(), \"training_output.txt\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        errors, true_xyz, predicted_xyz = evaluate_model(model, test_inputs, test_outputs, input_mean, input_std)\n",
    "        \n",
    "        # Log evaluation metrics\n",
    "        mlflow.log_metric(\"mean_error\", np.mean(errors))\n",
    "        mlflow.log_metric(\"median_error\", np.median(errors))\n",
    "        mlflow.log_metric(\"90th_percentile_error\", np.percentile(errors, 90))\n",
    "        mlflow.log_metric(\"max_error\", np.max(errors))\n",
    "        \n",
    "        # Generate and log the true vs predicted plot\n",
    "        true_vs_pred_plot_path = f\"{config['model_name']}_true_vs_predicted.png\"\n",
    "        plot_true_vs_predicted(true_xyz, predicted_xyz, f\"{config['model_name']} Model: True vs Predicted\", save_path=true_vs_pred_plot_path)\n",
    "        mlflow.log_artifact(true_vs_pred_plot_path)\n",
    "        os.remove(true_vs_pred_plot_path)  # Clean up the temporary file\n",
    "        \n",
    "        # Generate and log the error distribution plot\n",
    "        error_dist_plot_path = f\"{config['model_name']}_error_distribution.png\"\n",
    "        plot_error_distribution(errors, f\"{config['model_name']} Model: Error Distribution\", save_path=error_dist_plot_path)\n",
    "        mlflow.log_artifact(error_dist_plot_path)\n",
    "        os.remove(error_dist_plot_path)  # Clean up the temporary file\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\n{config['model_name']} Model:\")\n",
    "        print(f\"Mean Error: {np.mean(errors):.4f}\")\n",
    "        print(f\"Median Error: {np.median(errors):.4f}\")\n",
    "        print(f\"90th Percentile Error: {np.percentile(errors, 90):.4f}\")\n",
    "        print(f\"Max Error: {np.max(errors):.4f}\")\n",
    "        print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'errors': errors,\n",
    "            'true_xyz': true_xyz,\n",
    "            'predicted_xyz': predicted_xyz,\n",
    "            'training_time': training_time\n",
    "        }\n",
    "\n",
    "def run_single_experiment(config):\n",
    "    (train_inputs, train_outputs), (test_inputs, test_outputs), input_mean, input_std = load_and_preprocess_data()\n",
    "    \n",
    "    # Split test data into validation and test sets\n",
    "    valid_inputs, test_inputs, valid_outputs, test_outputs = train_test_split(\n",
    "        test_inputs, test_outputs, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_and_evaluate_model(\n",
    "        (train_inputs, train_outputs),\n",
    "        (valid_inputs, valid_outputs),\n",
    "        (test_inputs, test_outputs),\n",
    "        input_mean, input_std,\n",
    "        config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 317, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 410, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1341, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1334, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/utils/file_utils.py\", line 309, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.5685Epoch 1/200 - loss: 0.5637 - val_loss: 0.2780\n",
      "138/138 [==============================] - 3s 11ms/step - loss: 0.5637 - val_loss: 0.2780\n",
      "Epoch 2/200\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.1803Epoch 2/200 - loss: 0.1803 - val_loss: 0.1253\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.1803 - val_loss: 0.1253\n",
      "Epoch 3/200\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0969Epoch 3/200 - loss: 0.0967 - val_loss: 0.0736\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0967 - val_loss: 0.0736\n",
      "Epoch 4/200\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0630Epoch 4/200 - loss: 0.0628 - val_loss: 0.0549\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0628 - val_loss: 0.0549\n",
      "Epoch 5/200\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0507Epoch 5/200 - loss: 0.0506 - val_loss: 0.0472\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0506 - val_loss: 0.0472\n",
      "Epoch 6/200\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0449Epoch 6/200 - loss: 0.0449 - val_loss: 0.0427\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0449 - val_loss: 0.0427\n",
      "Epoch 7/200\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0415Epoch 7/200 - loss: 0.0414 - val_loss: 0.0398\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0414 - val_loss: 0.0398\n",
      "Epoch 8/200\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0384Epoch 8/200 - loss: 0.0384 - val_loss: 0.0400\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0384 - val_loss: 0.0400\n",
      "Epoch 9/200\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0345Epoch 9/200 - loss: 0.0345 - val_loss: 0.0317\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0345 - val_loss: 0.0317\n",
      "Epoch 10/200\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0303Epoch 10/200 - loss: 0.0303 - val_loss: 0.0320\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0303 - val_loss: 0.0320\n",
      "Epoch 11/200\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0260Epoch 11/200 - loss: 0.0260 - val_loss: 0.0269\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0260 - val_loss: 0.0269\n",
      "Epoch 12/200\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0230Epoch 12/200 - loss: 0.0230 - val_loss: 0.0214\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0230 - val_loss: 0.0214\n",
      "Epoch 13/200\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0215Epoch 13/200 - loss: 0.0215 - val_loss: 0.0209\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0215 - val_loss: 0.0209\n",
      "Epoch 14/200\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0207Epoch 14/200 - loss: 0.0207 - val_loss: 0.0197\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0207 - val_loss: 0.0197\n",
      "Epoch 15/200\n",
      "132/138 [===========================>..] - ETA: 0s - loss: 0.0198Epoch 15/200 - loss: 0.0198 - val_loss: 0.0198\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0198 - val_loss: 0.0198\n",
      "Epoch 16/200\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0194Epoch 16/200 - loss: 0.0194 - val_loss: 0.0197\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0194 - val_loss: 0.0197\n",
      "Epoch 17/200\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0187Epoch 17/200 - loss: 0.0187 - val_loss: 0.0186\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0187 - val_loss: 0.0186\n",
      "Epoch 18/200\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0184Epoch 18/200 - loss: 0.0183 - val_loss: 0.0177\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0183 - val_loss: 0.0177\n",
      "Epoch 19/200\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0183Epoch 19/200 - loss: 0.0183 - val_loss: 0.0173\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0183 - val_loss: 0.0173\n",
      "Epoch 20/200\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0177Epoch 20/200 - loss: 0.0177 - val_loss: 0.0170\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0177 - val_loss: 0.0170\n",
      "Epoch 21/200\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0172Epoch 21/200 - loss: 0.0172 - val_loss: 0.0182\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0172 - val_loss: 0.0182\n",
      "Epoch 22/200\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0169Epoch 22/200 - loss: 0.0169 - val_loss: 0.0166\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0169 - val_loss: 0.0166\n",
      "Epoch 23/200\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0168Epoch 23/200 - loss: 0.0168 - val_loss: 0.0166\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0168 - val_loss: 0.0166\n",
      "Epoch 24/200\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0165Epoch 24/200 - loss: 0.0165 - val_loss: 0.0163\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0165 - val_loss: 0.0163\n",
      "Epoch 25/200\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0164Epoch 25/200 - loss: 0.0164 - val_loss: 0.0158\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0164 - val_loss: 0.0158\n",
      "Epoch 26/200\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0162Epoch 26/200 - loss: 0.0162 - val_loss: 0.0164\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0162 - val_loss: 0.0164\n",
      "Epoch 27/200\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0164Epoch 27/200 - loss: 0.0164 - val_loss: 0.0159\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0164 - val_loss: 0.0159\n",
      "Epoch 28/200\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0162Epoch 28/200 - loss: 0.0162 - val_loss: 0.0158\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0162 - val_loss: 0.0158\n",
      "Epoch 29/200\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0160Epoch 29/200 - loss: 0.0160 - val_loss: 0.0157\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0160 - val_loss: 0.0157\n",
      "Epoch 30/200\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0160Epoch 30/200 - loss: 0.0160 - val_loss: 0.0155\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0160 - val_loss: 0.0155\n",
      "Epoch 31/200\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0159Epoch 31/200 - loss: 0.0159 - val_loss: 0.0155\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0159 - val_loss: 0.0155\n",
      "Epoch 32/200\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0158Epoch 32/200 - loss: 0.0158 - val_loss: 0.0157\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0158 - val_loss: 0.0157\n",
      "Epoch 33/200\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0158Epoch 33/200 - loss: 0.0158 - val_loss: 0.0155\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0158 - val_loss: 0.0155\n",
      "Epoch 34/200\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0158Epoch 34/200 - loss: 0.0158 - val_loss: 0.0153\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0158 - val_loss: 0.0153\n",
      "Epoch 35/200\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0157Epoch 35/200 - loss: 0.0157 - val_loss: 0.0161\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0157 - val_loss: 0.0161\n",
      "Epoch 36/200\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0157Epoch 36/200 - loss: 0.0157 - val_loss: 0.0157\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 37/200\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0157Epoch 37/200 - loss: 0.0157 - val_loss: 0.0151\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0157 - val_loss: 0.0151\n",
      "Epoch 38/200\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0155Epoch 38/200 - loss: 0.0155 - val_loss: 0.0169\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0155 - val_loss: 0.0169\n",
      "Epoch 39/200\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0156Epoch 39/200 - loss: 0.0156 - val_loss: 0.0156\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 40/200\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0156Epoch 40/200 - loss: 0.0156 - val_loss: 0.0154\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0156 - val_loss: 0.0154\n",
      "Epoch 41/200\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0156Epoch 41/200 - loss: 0.0156 - val_loss: 0.0155\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0156 - val_loss: 0.0155\n",
      "Epoch 42/200\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0155Epoch 42/200 - loss: 0.0155 - val_loss: 0.0158\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0155 - val_loss: 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/20 01:57:26 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmprnah6waq/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmprnah6waq/model/data/model/assets\n",
      "2024/08/20 01:57:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmprnah6waq/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "/usr/local/lib/python3.8/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2024/08/20 01:57:30 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n",
      "\n",
      "Huber Loss (both), fk=10e200,d01 Model:\n",
      "Mean Error: 0.0356\n",
      "Median Error: 0.0349\n",
      "90th Percentile Error: 0.0526\n",
      "Max Error: 3.9077\n",
      "Training Time: 57.15 seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        \"model_name\": \"Huber Loss (both), fk=10e200,d01\",\n",
    "        \"loss_type\": \"modified_custom\",\n",
    "        \"fk_weight\": 10,\n",
    "        \"epochs\": epochs,\n",
    "        \"initial_learning_rate\": 1e-3,\n",
    "        \"batch_size\": 65536,\n",
    "        \"experiment_name\": \"Inverse Kinematics NN Comparison\"\n",
    "    }\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for config in configs:\n",
    "    results[config['model_name']] = run_single_experiment(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': <keras.engine.sequential.Sequential at 0x7efbb666b610>,\n",
       " 'history': <keras.callbacks.History at 0x7efca8512cd0>,\n",
       " 'errors': array([0.02859872, 0.02799114, 0.02627422, ..., 0.01310541, 0.03912053,\n",
       "        0.02646932], dtype=float32),\n",
       " 'true_xyz': array([[ 0.6068389 , -1.5461596 ,  1.9428234 ],\n",
       "        [-1.2951391 ,  1.9623394 ,  1.2530879 ],\n",
       "        [-1.8338947 ,  1.0807508 ,  1.7556641 ],\n",
       "        ...,\n",
       "        [ 0.52119195, -1.9730337 , -0.3695886 ],\n",
       "        [ 0.19297831,  1.5389277 , -0.6839478 ],\n",
       "        [-1.3054239 ,  1.4633682 ,  0.14486456]], dtype=float32),\n",
       " 'predicted_xyz': array([[ 0.6174133 , -1.5723236 ,  1.9474621 ],\n",
       "        [-1.3103077 ,  1.9816096 ,  1.2665814 ],\n",
       "        [-1.8496726 ,  1.0984539 ,  1.7669777 ],\n",
       "        ...,\n",
       "        [ 0.5119995 , -1.9813952 , -0.37375206],\n",
       "        [ 0.20352566,  1.573889  , -0.6699166 ],\n",
       "        [-1.2805868 ,  1.4610975 ,  0.13599938]], dtype=float32),\n",
       " 'training_time': 57.15261268615723}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[configs[0]['model_name']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
