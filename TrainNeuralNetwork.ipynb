{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io, os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_dataset(filename='robot_arm_dataset_10M.npz'):\n",
    "    data = np.load(f'./Data/{filename}')\n",
    "    return data['inputs'], data['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_model(input_shape, output_shape):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        keras.layers.Dense(128),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(64),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(output_shape)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseLoggingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = io.StringIO()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        output = f\"Epoch {epoch+1}/{self.params['epochs']} - \"\n",
    "        output += \" - \".join(f\"{k}: {v:.4f}\" for k, v in logs.items())\n",
    "        print(output)\n",
    "        self.output.write(output + \"\\n\")\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.output.getvalue()\n",
    "    \n",
    "class LearningRateLogger(keras.callbacks.Callback):\n",
    "    def __init__(self, tensorboard_writer):\n",
    "        super().__init__()\n",
    "        self.tensorboard_writer = tensorboard_writer\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'value'):\n",
    "            lr = lr.value()\n",
    "        with self.tensorboard_writer.as_default():\n",
    "            tf.summary.scalar('learning_rate', data=lr, step=epoch)\n",
    "        mlflow.log_metric(\"learning_rate\", lr, step=epoch)\n",
    "        \n",
    "class CosineDecayWithWarmupCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_learning_rate, warmup_steps, total_steps):\n",
    "        super(CosineDecayWithWarmupCallback, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            lr = self.initial_learning_rate * (self.current_step / self.warmup_steps)\n",
    "        else:\n",
    "            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            lr = 0.5 * self.initial_learning_rate * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        self.current_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(batch_size, epochs, initial_learning_rate, test_size=0.2, experiment_name=\"Inverse Kinematics NN\", run_name=None):\n",
    "    # Set up MLflow\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    # Generate a unique run name if one is provided\n",
    "    if run_name:\n",
    "        timestamp = int(time.time())\n",
    "        unique_run_name = f\"{run_name}_{timestamp}\"\n",
    "    else:\n",
    "        unique_run_name = None\n",
    "        \n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # Create a consistent directory structure for TensorBoard logs\n",
    "        run_id = run.info.run_id\n",
    "        run_name = run.data.tags.get('mlflow.runName', run_id)\n",
    "        log_dir = os.path.join(\"logs\", experiment_name, f\"{run_name}_{run_id}\")\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "        # Load and split the data\n",
    "        X, y = load_dataset()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"initial_learning_rate\", initial_learning_rate)\n",
    "        mlflow.log_param(\"test_size\", test_size)\n",
    "\n",
    "        # Calculate total steps\n",
    "        steps_per_epoch = len(X_train) // batch_size\n",
    "        total_steps = steps_per_epoch * epochs\n",
    "        warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n",
    "        \n",
    "        # Create and compile the model\n",
    "        model = create_model(input_shape=(3,), output_shape=3)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Log model summary\n",
    "        model_summary = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: model_summary.write(x + '\\n'))\n",
    "        mlflow.log_text(model_summary.getvalue(), \"model_summary.txt\")\n",
    "\n",
    "        # Set up TensorBoard callback and writer\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        tensorboard_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "        # Set up other callbacks\n",
    "        early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        lr_scheduler = CosineDecayWithWarmupCallback(initial_learning_rate, warmup_steps, total_steps)\n",
    "        lr_logger = LearningRateLogger(tensorboard_writer)\n",
    "        \n",
    "        callbacks = [tensorboard_callback, lr_scheduler, lr_logger]\n",
    "\n",
    "        # Log callback names\n",
    "        callback_names = [callback.__class__.__name__ for callback in callbacks]\n",
    "        mlflow.log_param(\"callbacks\", \", \".join(callback_names))\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Log metrics\n",
    "        for epoch, (loss, val_loss) in enumerate(zip(\n",
    "            history.history['loss'],\n",
    "            history.history['val_loss']\n",
    "        )):\n",
    "            mlflow.log_metric(\"train_loss\", loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "        # Log the TensorBoard log directory\n",
    "        mlflow.log_param(\"tensorboard_log_dir\", log_dir)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "\n",
    "    print(\"Training completed and logged with MLflow and TensorBoard.\")\n",
    "    print(f\"Experiment name: {experiment_name}\")\n",
    "    print(f\"Run name: {run_name}\")\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"TensorBoard logs saved to: {log_dir}\")\n",
    "    print(\"To view in TensorBoard, run:\")\n",
    "    print(f\"tensorboard --logdir logs/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/245 [..............................] - ETA: 5:56 - loss: 1.8341WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0047s vs `on_train_batch_end` time: 0.0058s). Check your callbacks.\n",
      "245/245 [==============================] - 4s 9ms/step - loss: 0.4320 - val_loss: 0.5198\n",
      "Epoch 2/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2600 - val_loss: 0.2975\n",
      "Epoch 3/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2489 - val_loss: 0.3078\n",
      "Epoch 4/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2408 - val_loss: 0.4709\n",
      "Epoch 5/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2421 - val_loss: 0.2971\n",
      "Epoch 6/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2357 - val_loss: 0.2736\n",
      "Epoch 7/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2328 - val_loss: 0.2698\n",
      "Epoch 8/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2301 - val_loss: 0.2501\n",
      "Epoch 9/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2285 - val_loss: 0.2636\n",
      "Epoch 10/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2290 - val_loss: 0.2486\n",
      "Epoch 11/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2284 - val_loss: 0.2685\n",
      "Epoch 12/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2269 - val_loss: 0.2389\n",
      "Epoch 13/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2257 - val_loss: 0.2582\n",
      "Epoch 14/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2256 - val_loss: 0.2387\n",
      "Epoch 15/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2241 - val_loss: 0.2383\n",
      "Epoch 16/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2235 - val_loss: 0.2341\n",
      "Epoch 17/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2229 - val_loss: 0.2633\n",
      "Epoch 18/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2232 - val_loss: 0.2338\n",
      "Epoch 19/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2225 - val_loss: 0.2493\n",
      "Epoch 20/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2220 - val_loss: 0.2332\n",
      "Epoch 21/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2211 - val_loss: 0.2641\n",
      "Epoch 22/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2218 - val_loss: 0.2255\n",
      "Epoch 23/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2201 - val_loss: 0.2210\n",
      "Epoch 24/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2196 - val_loss: 0.2215\n",
      "Epoch 25/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2199 - val_loss: 0.2187\n",
      "Epoch 26/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2189 - val_loss: 0.2219\n",
      "Epoch 27/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2193 - val_loss: 0.2210\n",
      "Epoch 28/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2176 - val_loss: 0.2214\n",
      "Epoch 29/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2185 - val_loss: 0.2189\n",
      "Epoch 30/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2181 - val_loss: 0.2176\n",
      "Epoch 31/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2179 - val_loss: 0.2274\n",
      "Epoch 32/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2173 - val_loss: 0.2181\n",
      "Epoch 33/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2168 - val_loss: 0.2208\n",
      "Epoch 34/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2175 - val_loss: 0.2156\n",
      "Epoch 35/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2160 - val_loss: 0.2172\n",
      "Epoch 36/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2161 - val_loss: 0.2160\n",
      "Epoch 37/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2158 - val_loss: 0.2153\n",
      "Epoch 38/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2160 - val_loss: 0.2134\n",
      "Epoch 39/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2158 - val_loss: 0.2141\n",
      "Epoch 40/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2148 - val_loss: 0.2112\n",
      "Epoch 41/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2145 - val_loss: 0.2110\n",
      "Epoch 42/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2144 - val_loss: 0.2126\n",
      "Epoch 43/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2141 - val_loss: 0.2117\n",
      "Epoch 44/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2142 - val_loss: 0.2106\n",
      "Epoch 45/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2137 - val_loss: 0.2098\n",
      "Epoch 46/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2129 - val_loss: 0.2095\n",
      "Epoch 47/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2130 - val_loss: 0.2094\n",
      "Epoch 48/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2134 - val_loss: 0.2095\n",
      "Epoch 49/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2131 - val_loss: 0.2092\n",
      "Epoch 50/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2126 - val_loss: 0.2092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 18:18:04 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptyt4ykpt/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2024/08/16 18:18:09 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN\n",
      "Run name: CosineWithWarmUp\n",
      "Run ID: 293b6c3deedc45e2862e85d2ef8bb208\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN/CosineWithWarmUp_293b6c3deedc45e2862e85d2ef8bb208\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    batch_size=2**15,\n",
    "    epochs=50,  # Increased epochs to demonstrate early stopping\n",
    "    initial_learning_rate=0.03,\n",
    "    test_size=0.2,\n",
    "    run_name=\"CosineWithWarmUp\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
