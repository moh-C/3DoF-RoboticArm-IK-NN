{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "L1, L2, L3 = 1.0, 1.5, 0.5  # link lengths\n",
    "\n",
    "@tf.function\n",
    "def forward_kinematics_tf(theta):\n",
    "    theta1, theta2, theta3 = tf.unstack(theta, axis=1)\n",
    "    \n",
    "    x = L1 * tf.cos(theta1) * tf.sin(theta2) + L2 * tf.cos(theta1) * tf.sin(theta2 + theta3)\n",
    "    y = L1 * tf.sin(theta1) * tf.sin(theta2) + L2 * tf.sin(theta1) * tf.sin(theta2 + theta3)\n",
    "    z = L1 * tf.cos(theta2) + L2 * tf.cos(theta2 + theta3) + L3\n",
    "    \n",
    "    return tf.stack([x, y, z], axis=1)\n",
    "\n",
    "def evaluate_model(model, test_inputs, test_outputs, input_mean, input_std, batch_size=2**16):\n",
    "    test_inputs = tf.convert_to_tensor(test_inputs, dtype=tf.float32)\n",
    "    input_mean = tf.convert_to_tensor(input_mean, dtype=tf.float32)\n",
    "    input_std = tf.convert_to_tensor(input_std, dtype=tf.float32)\n",
    "    predicted_angles_normalized = model.predict(test_inputs, batch_size=batch_size)\n",
    "    predicted_angles = predicted_angles_normalized * (np.pi/2)\n",
    "    true_xyz = test_inputs * input_std + input_mean\n",
    "    predicted_xyz = forward_kinematics_tf(predicted_angles)\n",
    "    errors = tf.norm(true_xyz - predicted_xyz, axis=1)\n",
    "    return errors.numpy(), true_xyz.numpy(), predicted_xyz.numpy()\n",
    "\n",
    "\n",
    "def create_model(config):\n",
    "    return create_model_with_activations(config['activation_config'], config['fk_weight'])\n",
    "\n",
    "def plot_error_distribution(errors, title, save_path=None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(errors, bins=400, alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Error (Euclidean distance)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlim([0, 0.5])\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_true_vs_predicted(true_xyz, predicted_xyz, title, save_path=None):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    for i, coord in enumerate(['X', 'Y', 'Z']):\n",
    "        ax = axes[i]\n",
    "        ax.scatter(true_xyz[:, i], predicted_xyz[:, i], alpha=0.1)\n",
    "        ax.plot([true_xyz[:, i].min(), true_xyz[:, i].max()], [true_xyz[:, i].min(), true_xyz[:, i].max()], 'r--')\n",
    "        ax.set_xlabel(f'True {coord}')\n",
    "        ax.set_ylabel(f'Predicted {coord}')\n",
    "        ax.set_title(f'{coord} Coordinate: True vs Predicted')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def load_and_preprocess_data(filename='robot_arm_dataset_10M.npz'):\n",
    "    data = np.load(f'./Data/{filename}')\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    \n",
    "    input_mean = np.mean(inputs, axis=0)\n",
    "    input_std = np.std(inputs, axis=0)\n",
    "    inputs_normalized = (inputs - input_mean) / input_std\n",
    "\n",
    "    outputs_normalized = outputs / (np.pi/2)\n",
    "\n",
    "    split_index = int(0.9 * len(inputs))\n",
    "    train_inputs, test_inputs = inputs_normalized[:split_index], inputs_normalized[split_index:]\n",
    "    train_outputs, test_outputs = outputs_normalized[:split_index], outputs_normalized[split_index:]\n",
    "\n",
    "    return (train_inputs, train_outputs), (test_inputs, test_outputs), input_mean, input_std\n",
    "\n",
    "class VerboseLoggingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = io.StringIO()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        output = f\"Epoch {epoch+1}/{self.params['epochs']} - \"\n",
    "        output += \" - \".join(f\"{k}: {v:.4f}\" for k, v in logs.items())\n",
    "        print(output)\n",
    "        self.output.write(output + \"\\n\")\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.output.getvalue()\n",
    "\n",
    "class LearningRateLogger(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'value'):\n",
    "            lr = lr.value()\n",
    "        mlflow.log_metric(\"learning_rate\", lr, step=epoch)\n",
    "        \n",
    "class CosineDecayWithWarmupCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_learning_rate, warmup_steps, total_steps):\n",
    "        super(CosineDecayWithWarmupCallback, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            lr = self.initial_learning_rate * (self.current_step / self.warmup_steps)\n",
    "        else:\n",
    "            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            lr = 0.5 * self.initial_learning_rate * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        self.current_step += 1\n",
    "\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "def custom_loss(fk_weight):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Huber loss for joint angles\n",
    "        angle_loss = tf.keras.losses.Huber(delta=0.1)(y_true, y_pred)\n",
    "        \n",
    "        # Forward kinematics loss (using Huber loss)\n",
    "        fk_true = forward_kinematics_tf(y_true)\n",
    "        fk_pred = forward_kinematics_tf(y_pred)\n",
    "        fk_loss = tf.keras.losses.Huber(delta=0.1)(fk_true, fk_pred)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = angle_loss + fk_weight * fk_loss\n",
    "        return total_loss\n",
    "    return loss_fn\n",
    "\n",
    "def create_model_with_activations(activation_config, fk_weight):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(3,)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(activation_functions[activation_config[0]]),\n",
    "        \n",
    "        keras.layers.Dense(256),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(activation_functions[activation_config[1]]),\n",
    "        \n",
    "        keras.layers.Dense(256),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(activation_functions[activation_config[2]]),\n",
    "        \n",
    "        keras.layers.Dense(128),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation(activation_functions[activation_config[3]]),\n",
    "        \n",
    "        keras.layers.Dense(3, activation='tanh')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=custom_loss(fk_weight))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_single_experiment(config):\n",
    "    (train_inputs, train_outputs), (test_inputs, test_outputs), input_mean, input_std = load_and_preprocess_data()\n",
    "    \n",
    "    # Split test data into validation and test sets\n",
    "    valid_inputs, test_inputs, valid_outputs, test_outputs = train_test_split(\n",
    "        test_inputs, test_outputs, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    mlflow.set_experiment(config['experiment_name'])\n",
    "    \n",
    "    with mlflow.start_run(run_name=config['model_name']):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(config)\n",
    "        \n",
    "        # Calculate total steps\n",
    "        steps_per_epoch = len(train_inputs) // config['batch_size']\n",
    "        total_steps = steps_per_epoch * config['epochs']\n",
    "        warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = create_model(config)\n",
    "        \n",
    "        # Log model summary\n",
    "        model_summary = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: model_summary.write(x + '\\n'))\n",
    "        mlflow.log_text(model_summary.getvalue(), \"model_summary.txt\")\n",
    "        \n",
    "        # Set up callbacks\n",
    "        early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        lr_scheduler = CosineDecayWithWarmupCallback(\n",
    "            config['initial_learning_rate'],\n",
    "            warmup_steps,\n",
    "            total_steps\n",
    "        )\n",
    "        verbose_logging = VerboseLoggingCallback()\n",
    "        lr_logger = LearningRateLogger()\n",
    "        \n",
    "        callbacks = [lr_scheduler, verbose_logging, lr_logger]\n",
    "        \n",
    "        # Log callback names\n",
    "        callback_names = [callback.__class__.__name__ for callback in callbacks]\n",
    "        mlflow.log_param(\"callbacks\", \", \".join(callback_names))\n",
    "        \n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            train_inputs, train_outputs,\n",
    "            epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            validation_data=(valid_inputs, valid_outputs),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Log training metrics\n",
    "        for epoch, (loss, val_loss) in enumerate(zip(history.history['loss'], history.history['val_loss'])):\n",
    "            mlflow.log_metric(\"train_loss\", loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        \n",
    "        mlflow.log_metric(\"training_time\", training_time)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        \n",
    "        # Log training output\n",
    "        mlflow.log_text(verbose_logging.get_output(), \"training_output.txt\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        errors, true_xyz, predicted_xyz = evaluate_model(model, test_inputs, test_outputs, input_mean, input_std)\n",
    "        \n",
    "        # Log evaluation metrics\n",
    "        mlflow.log_metric(\"mean_error\", np.mean(errors))\n",
    "        mlflow.log_metric(\"median_error\", np.median(errors))\n",
    "        mlflow.log_metric(\"90th_percentile_error\", np.percentile(errors, 90))\n",
    "        mlflow.log_metric(\"max_error\", np.max(errors))\n",
    "        \n",
    "        # Generate and log the true vs predicted plot\n",
    "        true_vs_pred_plot_path = f\"./Figures/ActivationFunction/{config['model_name']}_true_vs_predicted.png\"\n",
    "        plot_true_vs_predicted(true_xyz, predicted_xyz, f\"{config['model_name']} Model: True vs Predicted\", save_path=true_vs_pred_plot_path)\n",
    "        mlflow.log_artifact(true_vs_pred_plot_path)\n",
    "        \n",
    "        # Generate and log the error distribution plot\n",
    "        error_dist_plot_path = f\"./Figures/ActivationFunction/{config['model_name']}_error_distribution.png\"\n",
    "        plot_error_distribution(errors, f\"{config['model_name']} Model: Error Distribution\", save_path=error_dist_plot_path)\n",
    "        mlflow.log_artifact(error_dist_plot_path)\n",
    "        \n",
    "        print(f\"\\n{config['model_name']} Model:\")\n",
    "        print(f\"Mean Error: {np.mean(errors):.4f}\")\n",
    "        print(f\"Median Error: {np.median(errors):.4f}\")\n",
    "        print(f\"90th Percentile Error: {np.percentile(errors, 90):.4f}\")\n",
    "        print(f\"Max Error: {np.max(errors):.4f}\")\n",
    "        print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'errors': errors,\n",
    "            'true_xyz': true_xyz,\n",
    "            'predicted_xyz': predicted_xyz,\n",
    "            'training_time': training_time\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of configurations: 2\n",
      "Model: Model__e100_BN_1_relu_relu_relu_relu_lr_0.01, LR: 0.01\n",
      "Model: Model__e100_BN_1_relu_relu_relu_relu_lr_0.04, LR: 0.04\n"
     ]
    }
   ],
   "source": [
    "# Create configurations\n",
    "configs = []\n",
    "\n",
    "# Define activation functions\n",
    "activation_functions = {\n",
    "    'relu': tf.nn.relu,\n",
    "    'leaky_relu': tf.nn.leaky_relu,\n",
    "    'swish': tf.nn.swish,\n",
    "    'mish': mish,\n",
    "    'elu': tf.nn.elu,\n",
    "    'selu': tf.nn.selu  # Adding SELU\n",
    "}\n",
    "\n",
    "activation_combinations = [\n",
    "    ('relu', 'relu', 'relu', 'relu'),\n",
    "#     ('leaky_relu', 'leaky_relu', 'leaky_relu', 'leaky_relu'),\n",
    "#     ('swish', 'swish', 'swish', 'swish'),\n",
    "#     ('mish', 'mish', 'mish', 'mish'),\n",
    "#     ('elu', 'elu', 'elu', 'elu'),\n",
    "#     ('selu', 'selu', 'selu', 'selu'),  # Adding SELU combination\n",
    "#     ('relu', 'leaky_relu', 'swish', 'mish'),\n",
    "#     ('swish', 'mish', 'elu', 'relu'),\n",
    "#     ('mish', 'elu', 'relu', 'leaky_relu'),\n",
    "#     ('elu', 'relu', 'leaky_relu', 'swish'),\n",
    "#     ('leaky_relu', 'swish', 'mish', 'elu'),\n",
    "#     ('selu', 'relu', 'swish', 'elu'),  # Adding a mixed combination with SELU\n",
    "#     ('mish', 'selu', 'leaky_relu', 'swish')  # Another mixed combination with SELU\n",
    "]\n",
    "\n",
    "learning_rates = [1e-2, 4e-2]\n",
    "\n",
    "for idx, activation_combo in enumerate(activation_combinations):\n",
    "    for lr in learning_rates:\n",
    "        config = {\n",
    "            \"model_name\": f\"Model__e100_BN_{idx+1}_{'_'.join(activation_combo)}_lr_{lr}\",\n",
    "            \"fk_weight\": 10,\n",
    "            \"epochs\": 100,\n",
    "            \"initial_learning_rate\": lr,\n",
    "            \"batch_size\": 65536,\n",
    "            \"experiment_name\": \"Inverse Kinematics Activation FN\",\n",
    "            \"activation_config\": activation_combo\n",
    "        }\n",
    "        configs.append(config)\n",
    "\n",
    "# Print the total number of configurations\n",
    "print(f\"Total number of configurations: {len(configs)}\")\n",
    "\n",
    "# Optionally, print out all configurations to verify\n",
    "for config in configs:\n",
    "    print(f\"Model: {config['model_name']}, LR: {config['initial_learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: Model__e100_BN_1_relu_relu_relu_relu_lr_0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 317, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 410, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1341, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1334, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/utils/file_utils.py\", line 309, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.2216Epoch 1/100 - loss: 0.2203 - val_loss: 0.5154\n",
      "138/138 [==============================] - 5s 22ms/step - loss: 0.2203 - val_loss: 0.5154\n",
      "Epoch 2/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0728Epoch 2/100 - loss: 0.0727 - val_loss: 0.2429\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0727 - val_loss: 0.2429\n",
      "Epoch 3/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0571Epoch 3/100 - loss: 0.0572 - val_loss: 0.1216\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0572 - val_loss: 0.1216\n",
      "Epoch 4/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0497Epoch 4/100 - loss: 0.0498 - val_loss: 0.0686\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0498 - val_loss: 0.0686\n",
      "Epoch 5/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0459Epoch 5/100 - loss: 0.0459 - val_loss: 0.0715\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0459 - val_loss: 0.0715\n",
      "Epoch 6/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0418Epoch 6/100 - loss: 0.0417 - val_loss: 0.0553\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0417 - val_loss: 0.0553\n",
      "Epoch 7/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0350Epoch 7/100 - loss: 0.0351 - val_loss: 0.0443\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0351 - val_loss: 0.0443\n",
      "Epoch 8/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0361Epoch 8/100 - loss: 0.0361 - val_loss: 0.0456\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0361 - val_loss: 0.0456\n",
      "Epoch 9/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0378Epoch 9/100 - loss: 0.0378 - val_loss: 0.0456\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0378 - val_loss: 0.0456\n",
      "Epoch 10/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0344Epoch 10/100 - loss: 0.0344 - val_loss: 0.0533\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0344 - val_loss: 0.0533\n",
      "Epoch 11/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0273Epoch 11/100 - loss: 0.0273 - val_loss: 0.0455\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0273 - val_loss: 0.0455\n",
      "Epoch 12/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0235Epoch 12/100 - loss: 0.0235 - val_loss: 0.0316\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0235 - val_loss: 0.0316\n",
      "Epoch 13/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0229Epoch 13/100 - loss: 0.0228 - val_loss: 0.0253\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0228 - val_loss: 0.0253\n",
      "Epoch 14/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0221Epoch 14/100 - loss: 0.0221 - val_loss: 0.0264\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0221 - val_loss: 0.0264\n",
      "Epoch 15/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0213Epoch 15/100 - loss: 0.0213 - val_loss: 0.0261\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0213 - val_loss: 0.0261\n",
      "Epoch 16/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0218Epoch 16/100 - loss: 0.0218 - val_loss: 0.0242\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0218 - val_loss: 0.0242\n",
      "Epoch 17/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0198Epoch 17/100 - loss: 0.0197 - val_loss: 0.0232\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0197 - val_loss: 0.0232\n",
      "Epoch 18/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0199Epoch 18/100 - loss: 0.0199 - val_loss: 0.0272\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0199 - val_loss: 0.0272\n",
      "Epoch 19/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0211Epoch 19/100 - loss: 0.0211 - val_loss: 0.0247\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0211 - val_loss: 0.0247\n",
      "Epoch 20/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0201Epoch 20/100 - loss: 0.0201 - val_loss: 0.0218\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0201 - val_loss: 0.0218\n",
      "Epoch 21/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0195Epoch 21/100 - loss: 0.0195 - val_loss: 0.0194\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0195 - val_loss: 0.0194\n",
      "Epoch 22/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0196Epoch 22/100 - loss: 0.0196 - val_loss: 0.0205\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0196 - val_loss: 0.0205\n",
      "Epoch 23/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0190Epoch 23/100 - loss: 0.0190 - val_loss: 0.0224\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0190 - val_loss: 0.0224\n",
      "Epoch 24/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0191Epoch 24/100 - loss: 0.0191 - val_loss: 0.0209\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0191 - val_loss: 0.0209\n",
      "Epoch 25/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0187Epoch 25/100 - loss: 0.0187 - val_loss: 0.0198\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0187 - val_loss: 0.0198\n",
      "Epoch 26/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0184Epoch 26/100 - loss: 0.0184 - val_loss: 0.0191\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0184 - val_loss: 0.0191\n",
      "Epoch 27/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0189Epoch 27/100 - loss: 0.0189 - val_loss: 0.0203\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0189 - val_loss: 0.0203\n",
      "Epoch 28/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0185Epoch 28/100 - loss: 0.0185 - val_loss: 0.0213\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0185 - val_loss: 0.0213\n",
      "Epoch 29/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0183Epoch 29/100 - loss: 0.0183 - val_loss: 0.0191\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0183 - val_loss: 0.0191\n",
      "Epoch 30/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0184Epoch 30/100 - loss: 0.0184 - val_loss: 0.0223\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0184 - val_loss: 0.0223\n",
      "Epoch 31/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0185Epoch 31/100 - loss: 0.0185 - val_loss: 0.0224\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0185 - val_loss: 0.0224\n",
      "Epoch 32/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0177Epoch 32/100 - loss: 0.0177 - val_loss: 0.0210\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0177 - val_loss: 0.0210\n",
      "Epoch 33/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0179Epoch 33/100 - loss: 0.0179 - val_loss: 0.0198\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0179 - val_loss: 0.0198\n",
      "Epoch 34/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0177Epoch 34/100 - loss: 0.0177 - val_loss: 0.0190\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0177 - val_loss: 0.0190\n",
      "Epoch 35/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0177Epoch 35/100 - loss: 0.0177 - val_loss: 0.0205\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0177 - val_loss: 0.0205\n",
      "Epoch 36/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0179Epoch 36/100 - loss: 0.0179 - val_loss: 0.0190\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0179 - val_loss: 0.0190\n",
      "Epoch 37/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0176Epoch 37/100 - loss: 0.0176 - val_loss: 0.0207\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0176 - val_loss: 0.0207\n",
      "Epoch 38/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0173Epoch 38/100 - loss: 0.0173 - val_loss: 0.0205\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0173 - val_loss: 0.0205\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/138 [============================>.] - ETA: 0s - loss: 0.0174Epoch 39/100 - loss: 0.0174 - val_loss: 0.0185\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0174 - val_loss: 0.0185\n",
      "Epoch 40/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0174Epoch 40/100 - loss: 0.0174 - val_loss: 0.0190\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0174 - val_loss: 0.0190\n",
      "Epoch 41/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0174Epoch 41/100 - loss: 0.0174 - val_loss: 0.0195\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0174 - val_loss: 0.0195\n",
      "Epoch 42/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0174Epoch 42/100 - loss: 0.0174 - val_loss: 0.0196\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0174 - val_loss: 0.0196\n",
      "Epoch 43/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0172Epoch 43/100 - loss: 0.0172 - val_loss: 0.0209\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0172 - val_loss: 0.0209\n",
      "Epoch 44/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0172Epoch 44/100 - loss: 0.0172 - val_loss: 0.0182\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0172 - val_loss: 0.0182\n",
      "Epoch 45/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0170Epoch 45/100 - loss: 0.0170 - val_loss: 0.0182\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0170 - val_loss: 0.0182\n",
      "Epoch 46/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0172Epoch 46/100 - loss: 0.0172 - val_loss: 0.0198\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0172 - val_loss: 0.0198\n",
      "Epoch 47/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0170Epoch 47/100 - loss: 0.0170 - val_loss: 0.0187\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0170 - val_loss: 0.0187\n",
      "Epoch 48/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0172Epoch 48/100 - loss: 0.0172 - val_loss: 0.0188\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0172 - val_loss: 0.0188\n",
      "Epoch 49/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0171Epoch 49/100 - loss: 0.0171 - val_loss: 0.0191\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0171 - val_loss: 0.0191\n",
      "Epoch 50/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0167Epoch 50/100 - loss: 0.0167 - val_loss: 0.0181\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0167 - val_loss: 0.0181\n",
      "Epoch 51/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0167Epoch 51/100 - loss: 0.0167 - val_loss: 0.0174\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0167 - val_loss: 0.0174\n",
      "Epoch 52/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0165Epoch 52/100 - loss: 0.0165 - val_loss: 0.0167\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0165 - val_loss: 0.0167\n",
      "Epoch 53/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0167Epoch 53/100 - loss: 0.0167 - val_loss: 0.0189\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0167 - val_loss: 0.0189\n",
      "Epoch 54/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0166Epoch 54/100 - loss: 0.0166 - val_loss: 0.0175\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0166 - val_loss: 0.0175\n",
      "Epoch 55/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0166Epoch 55/100 - loss: 0.0166 - val_loss: 0.0176\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0166 - val_loss: 0.0176\n",
      "Epoch 56/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0169Epoch 56/100 - loss: 0.0169 - val_loss: 0.0188\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0169 - val_loss: 0.0188\n",
      "Epoch 57/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0164Epoch 57/100 - loss: 0.0164 - val_loss: 0.0165\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0164 - val_loss: 0.0165\n",
      "Epoch 58/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0165Epoch 58/100 - loss: 0.0165 - val_loss: 0.0187\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0165 - val_loss: 0.0187\n",
      "Epoch 59/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0164Epoch 59/100 - loss: 0.0164 - val_loss: 0.0177\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0164 - val_loss: 0.0177\n",
      "Epoch 60/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0163Epoch 60/100 - loss: 0.0163 - val_loss: 0.0161\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0163 - val_loss: 0.0161\n",
      "Epoch 61/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0163Epoch 61/100 - loss: 0.0163 - val_loss: 0.0166\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0163 - val_loss: 0.0166\n",
      "Epoch 62/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0163Epoch 62/100 - loss: 0.0163 - val_loss: 0.0164\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0163 - val_loss: 0.0164\n",
      "Epoch 63/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0162Epoch 63/100 - loss: 0.0162 - val_loss: 0.0167\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0162 - val_loss: 0.0167\n",
      "Epoch 64/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0159Epoch 64/100 - loss: 0.0159 - val_loss: 0.0158\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0159 - val_loss: 0.0158\n",
      "Epoch 65/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0159Epoch 65/100 - loss: 0.0159 - val_loss: 0.0174\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0159 - val_loss: 0.0174\n",
      "Epoch 66/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0160Epoch 66/100 - loss: 0.0160 - val_loss: 0.0165\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0160 - val_loss: 0.0165\n",
      "Epoch 67/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0160Epoch 67/100 - loss: 0.0160 - val_loss: 0.0157\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0160 - val_loss: 0.0157\n",
      "Epoch 68/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0159Epoch 68/100 - loss: 0.0159 - val_loss: 0.0162\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0159 - val_loss: 0.0162\n",
      "Epoch 69/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0159Epoch 69/100 - loss: 0.0159 - val_loss: 0.0160\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0159 - val_loss: 0.0160\n",
      "Epoch 70/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0158Epoch 70/100 - loss: 0.0158 - val_loss: 0.0160\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0158 - val_loss: 0.0160\n",
      "Epoch 71/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0158Epoch 71/100 - loss: 0.0158 - val_loss: 0.0157\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0158 - val_loss: 0.0157\n",
      "Epoch 72/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0158Epoch 72/100 - loss: 0.0157 - val_loss: 0.0156\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0157 - val_loss: 0.0156\n",
      "Epoch 73/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0158Epoch 73/100 - loss: 0.0158 - val_loss: 0.0155\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0158 - val_loss: 0.0155\n",
      "Epoch 74/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0157Epoch 74/100 - loss: 0.0157 - val_loss: 0.0157\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 75/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0157Epoch 75/100 - loss: 0.0157 - val_loss: 0.0158\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0157 - val_loss: 0.0158\n",
      "Epoch 76/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0157Epoch 76/100 - loss: 0.0157 - val_loss: 0.0155\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0157 - val_loss: 0.0155\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/138 [============================>.] - ETA: 0s - loss: 0.0155Epoch 77/100 - loss: 0.0155 - val_loss: 0.0153\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0155 - val_loss: 0.0153\n",
      "Epoch 78/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0155Epoch 78/100 - loss: 0.0155 - val_loss: 0.0155\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0155 - val_loss: 0.0155\n",
      "Epoch 79/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0155Epoch 79/100 - loss: 0.0155 - val_loss: 0.0151\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0155 - val_loss: 0.0151\n",
      "Epoch 80/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0153Epoch 80/100 - loss: 0.0153 - val_loss: 0.0150\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0153 - val_loss: 0.0150\n",
      "Epoch 81/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0154Epoch 81/100 - loss: 0.0154 - val_loss: 0.0153\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0154 - val_loss: 0.0153\n",
      "Epoch 82/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0154Epoch 82/100 - loss: 0.0154 - val_loss: 0.0150\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0154 - val_loss: 0.0150\n",
      "Epoch 83/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0153Epoch 83/100 - loss: 0.0153 - val_loss: 0.0150\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0153 - val_loss: 0.0150\n",
      "Epoch 84/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0152Epoch 84/100 - loss: 0.0152 - val_loss: 0.0151\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0152 - val_loss: 0.0151\n",
      "Epoch 85/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0153Epoch 85/100 - loss: 0.0153 - val_loss: 0.0149\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0153 - val_loss: 0.0149\n",
      "Epoch 86/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0153Epoch 86/100 - loss: 0.0153 - val_loss: 0.0150\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0153 - val_loss: 0.0150\n",
      "Epoch 87/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0152Epoch 87/100 - loss: 0.0152 - val_loss: 0.0152\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 88/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0152Epoch 88/100 - loss: 0.0152 - val_loss: 0.0148\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0152 - val_loss: 0.0148\n",
      "Epoch 89/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0151Epoch 89/100 - loss: 0.0151 - val_loss: 0.0147\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0151 - val_loss: 0.0147\n",
      "Epoch 90/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0151Epoch 90/100 - loss: 0.0151 - val_loss: 0.0148\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0151 - val_loss: 0.0148\n",
      "Epoch 91/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0151Epoch 91/100 - loss: 0.0151 - val_loss: 0.0147\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0151 - val_loss: 0.0147\n",
      "Epoch 92/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 92/100 - loss: 0.0150 - val_loss: 0.0146\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "Epoch 93/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0151Epoch 93/100 - loss: 0.0151 - val_loss: 0.0146\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0151 - val_loss: 0.0146\n",
      "Epoch 94/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 94/100 - loss: 0.0150 - val_loss: 0.0146\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "Epoch 95/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0151Epoch 95/100 - loss: 0.0151 - val_loss: 0.0146\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0151 - val_loss: 0.0146\n",
      "Epoch 96/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 96/100 - loss: 0.0150 - val_loss: 0.0145\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0150 - val_loss: 0.0145\n",
      "Epoch 97/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 97/100 - loss: 0.0150 - val_loss: 0.0146\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "Epoch 98/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 98/100 - loss: 0.0150 - val_loss: 0.0146\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "Epoch 99/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 99/100 - loss: 0.0150 - val_loss: 0.0145\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0150 - val_loss: 0.0145\n",
      "Epoch 100/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 100/100 - loss: 0.0150 - val_loss: 0.0145\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0150 - val_loss: 0.0145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/20 18:09:36 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9libcnvh/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9libcnvh/model/data/model/assets\n",
      "2024/08/20 18:09:42 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp9libcnvh/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "/usr/local/lib/python3.8/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2024/08/20 18:09:42 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step\n",
      "\n",
      "Model__e100_BN_1_relu_relu_relu_relu_lr_0.01 Model:\n",
      "Mean Error: 0.0259\n",
      "Median Error: 0.0263\n",
      "90th Percentile Error: 0.0436\n",
      "Max Error: 3.8259\n",
      "Training Time: 294.08 seconds\n",
      "Running experiment: Model__e100_BN_1_relu_relu_relu_relu_lr_0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 317, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 410, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1341, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1334, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/utils/file_utils.py\", line 309, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.1939Epoch 1/100 - loss: 0.1927 - val_loss: 0.3895\n",
      "138/138 [==============================] - 4s 22ms/step - loss: 0.1927 - val_loss: 0.3895\n",
      "Epoch 2/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0589Epoch 2/100 - loss: 0.0589 - val_loss: 0.1963\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0589 - val_loss: 0.1963\n",
      "Epoch 3/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0467Epoch 3/100 - loss: 0.0467 - val_loss: 0.0621\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0467 - val_loss: 0.0621\n",
      "Epoch 4/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0435Epoch 4/100 - loss: 0.0434 - val_loss: 0.0572\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0434 - val_loss: 0.0572\n",
      "Epoch 5/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0351Epoch 5/100 - loss: 0.0350 - val_loss: 0.0606\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0350 - val_loss: 0.0606\n",
      "Epoch 6/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0322Epoch 6/100 - loss: 0.0322 - val_loss: 0.0754\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0322 - val_loss: 0.0754\n",
      "Epoch 7/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0302Epoch 7/100 - loss: 0.0302 - val_loss: 0.0658\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0302 - val_loss: 0.0658\n",
      "Epoch 8/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0264Epoch 8/100 - loss: 0.0264 - val_loss: 0.0565\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0264 - val_loss: 0.0565\n",
      "Epoch 9/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0238Epoch 9/100 - loss: 0.0237 - val_loss: 0.0505\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0237 - val_loss: 0.0505\n",
      "Epoch 10/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0240Epoch 10/100 - loss: 0.0240 - val_loss: 0.0466\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0240 - val_loss: 0.0466\n",
      "Epoch 11/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0221Epoch 11/100 - loss: 0.0221 - val_loss: 0.0361\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0221 - val_loss: 0.0361\n",
      "Epoch 12/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0206Epoch 12/100 - loss: 0.0206 - val_loss: 0.0261\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0206 - val_loss: 0.0261\n",
      "Epoch 13/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0209Epoch 13/100 - loss: 0.0209 - val_loss: 0.0245\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0209 - val_loss: 0.0245\n",
      "Epoch 14/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0198Epoch 14/100 - loss: 0.0198 - val_loss: 0.0246\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0198 - val_loss: 0.0246\n",
      "Epoch 15/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0198Epoch 15/100 - loss: 0.0198 - val_loss: 0.0315\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0198 - val_loss: 0.0315\n",
      "Epoch 16/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0201Epoch 16/100 - loss: 0.0201 - val_loss: 0.0313\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0201 - val_loss: 0.0313\n",
      "Epoch 17/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0189Epoch 17/100 - loss: 0.0189 - val_loss: 0.0315\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0189 - val_loss: 0.0315\n",
      "Epoch 18/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0185Epoch 18/100 - loss: 0.0186 - val_loss: 0.0273\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0186 - val_loss: 0.0273\n",
      "Epoch 19/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0191Epoch 19/100 - loss: 0.0191 - val_loss: 0.0355\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0191 - val_loss: 0.0355\n",
      "Epoch 20/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0186Epoch 20/100 - loss: 0.0186 - val_loss: 0.0313\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0186 - val_loss: 0.0313\n",
      "Epoch 21/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0188Epoch 21/100 - loss: 0.0188 - val_loss: 0.0261\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0188 - val_loss: 0.0261\n",
      "Epoch 22/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0182Epoch 22/100 - loss: 0.0182 - val_loss: 0.0272\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0182 - val_loss: 0.0272\n",
      "Epoch 23/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0189Epoch 23/100 - loss: 0.0188 - val_loss: 0.0367\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0188 - val_loss: 0.0367\n",
      "Epoch 24/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0186Epoch 24/100 - loss: 0.0186 - val_loss: 0.0327\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0186 - val_loss: 0.0327\n",
      "Epoch 25/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0187Epoch 25/100 - loss: 0.0187 - val_loss: 0.0264\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0187 - val_loss: 0.0264\n",
      "Epoch 26/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0178Epoch 26/100 - loss: 0.0178 - val_loss: 0.0265\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0178 - val_loss: 0.0265\n",
      "Epoch 27/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0184Epoch 27/100 - loss: 0.0184 - val_loss: 0.0311\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0184 - val_loss: 0.0311\n",
      "Epoch 28/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0178Epoch 28/100 - loss: 0.0178 - val_loss: 0.0263\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0178 - val_loss: 0.0263\n",
      "Epoch 29/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0185Epoch 29/100 - loss: 0.0185 - val_loss: 0.0319\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0185 - val_loss: 0.0319\n",
      "Epoch 30/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0183Epoch 30/100 - loss: 0.0183 - val_loss: 0.0234\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0183 - val_loss: 0.0234\n",
      "Epoch 31/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0181Epoch 31/100 - loss: 0.0181 - val_loss: 0.0237\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0181 - val_loss: 0.0237\n",
      "Epoch 32/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0182Epoch 32/100 - loss: 0.0182 - val_loss: 0.0266\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0182 - val_loss: 0.0266\n",
      "Epoch 33/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0181Epoch 33/100 - loss: 0.0181 - val_loss: 0.0253\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0181 - val_loss: 0.0253\n",
      "Epoch 34/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0178Epoch 34/100 - loss: 0.0178 - val_loss: 0.0232\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0178 - val_loss: 0.0232\n",
      "Epoch 35/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0175Epoch 35/100 - loss: 0.0175 - val_loss: 0.0238\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0175 - val_loss: 0.0238\n",
      "Epoch 36/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0176Epoch 36/100 - loss: 0.0176 - val_loss: 0.0297\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0176 - val_loss: 0.0297\n",
      "Epoch 37/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0178Epoch 37/100 - loss: 0.0178 - val_loss: 0.0252\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0178 - val_loss: 0.0252\n",
      "Epoch 38/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0175Epoch 38/100 - loss: 0.0175 - val_loss: 0.0218\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0175 - val_loss: 0.0218\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/138 [============================>.] - ETA: 0s - loss: 0.0175Epoch 39/100 - loss: 0.0175 - val_loss: 0.0220\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0175 - val_loss: 0.0220\n",
      "Epoch 40/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0174Epoch 40/100 - loss: 0.0174 - val_loss: 0.0277\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0174 - val_loss: 0.0277\n",
      "Epoch 41/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0179Epoch 41/100 - loss: 0.0178 - val_loss: 0.0255\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0178 - val_loss: 0.0255\n",
      "Epoch 42/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0171Epoch 42/100 - loss: 0.0171 - val_loss: 0.0250\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0171 - val_loss: 0.0250\n",
      "Epoch 43/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0181Epoch 43/100 - loss: 0.0180 - val_loss: 0.0273\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0180 - val_loss: 0.0273\n",
      "Epoch 44/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0175Epoch 44/100 - loss: 0.0174 - val_loss: 0.0203\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0174 - val_loss: 0.0203\n",
      "Epoch 45/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0171Epoch 45/100 - loss: 0.0171 - val_loss: 0.0209\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0171 - val_loss: 0.0209\n",
      "Epoch 46/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0172Epoch 46/100 - loss: 0.0172 - val_loss: 0.0251\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0172 - val_loss: 0.0251\n",
      "Epoch 47/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0172Epoch 47/100 - loss: 0.0172 - val_loss: 0.0547\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0172 - val_loss: 0.0547\n",
      "Epoch 48/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0171Epoch 48/100 - loss: 0.0171 - val_loss: 0.0195\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0171 - val_loss: 0.0195\n",
      "Epoch 49/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0170Epoch 49/100 - loss: 0.0170 - val_loss: 0.0220\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0170 - val_loss: 0.0220\n",
      "Epoch 50/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0169Epoch 50/100 - loss: 0.0169 - val_loss: 0.0187\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0169 - val_loss: 0.0187\n",
      "Epoch 51/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0172Epoch 51/100 - loss: 0.0172 - val_loss: 0.0215\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0172 - val_loss: 0.0215\n",
      "Epoch 52/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0168Epoch 52/100 - loss: 0.0168 - val_loss: 0.0194\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0168 - val_loss: 0.0194\n",
      "Epoch 53/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0168Epoch 53/100 - loss: 0.0168 - val_loss: 0.0216\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0168 - val_loss: 0.0216\n",
      "Epoch 54/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0167Epoch 54/100 - loss: 0.0167 - val_loss: 0.0169\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 55/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0165Epoch 55/100 - loss: 0.0165 - val_loss: 0.0218\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0165 - val_loss: 0.0218\n",
      "Epoch 56/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0166Epoch 56/100 - loss: 0.0166 - val_loss: 0.0190\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0166 - val_loss: 0.0190\n",
      "Epoch 57/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0163Epoch 57/100 - loss: 0.0163 - val_loss: 0.0170\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0163 - val_loss: 0.0170\n",
      "Epoch 58/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0164Epoch 58/100 - loss: 0.0164 - val_loss: 0.0195\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0164 - val_loss: 0.0195\n",
      "Epoch 59/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0164Epoch 59/100 - loss: 0.0164 - val_loss: 0.0174\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0164 - val_loss: 0.0174\n",
      "Epoch 60/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0165Epoch 60/100 - loss: 0.0165 - val_loss: 0.0217\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0165 - val_loss: 0.0217\n",
      "Epoch 61/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0162Epoch 61/100 - loss: 0.0162 - val_loss: 0.0178\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0162 - val_loss: 0.0178\n",
      "Epoch 62/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0163Epoch 62/100 - loss: 0.0163 - val_loss: 0.0189\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0163 - val_loss: 0.0189\n",
      "Epoch 63/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0165Epoch 63/100 - loss: 0.0165 - val_loss: 0.0178\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0165 - val_loss: 0.0178\n",
      "Epoch 64/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0161Epoch 64/100 - loss: 0.0161 - val_loss: 0.0165\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0161 - val_loss: 0.0165\n",
      "Epoch 65/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0162Epoch 65/100 - loss: 0.0162 - val_loss: 0.0166\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0162 - val_loss: 0.0166\n",
      "Epoch 66/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0161Epoch 66/100 - loss: 0.0161 - val_loss: 0.0178\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0161 - val_loss: 0.0178\n",
      "Epoch 67/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0161Epoch 67/100 - loss: 0.0162 - val_loss: 0.0169\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0162 - val_loss: 0.0169\n",
      "Epoch 68/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0160Epoch 68/100 - loss: 0.0160 - val_loss: 0.0170\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0160 - val_loss: 0.0170\n",
      "Epoch 69/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0160Epoch 69/100 - loss: 0.0160 - val_loss: 0.0170\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0160 - val_loss: 0.0170\n",
      "Epoch 70/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0158Epoch 70/100 - loss: 0.0158 - val_loss: 0.0167\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0158 - val_loss: 0.0167\n",
      "Epoch 71/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0159Epoch 71/100 - loss: 0.0159 - val_loss: 0.0163\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0159 - val_loss: 0.0163\n",
      "Epoch 72/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0158Epoch 72/100 - loss: 0.0157 - val_loss: 0.0166\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0157 - val_loss: 0.0166\n",
      "Epoch 73/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0156Epoch 73/100 - loss: 0.0156 - val_loss: 0.0153\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0156 - val_loss: 0.0153\n",
      "Epoch 74/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0155Epoch 74/100 - loss: 0.0155 - val_loss: 0.0164\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0155 - val_loss: 0.0164\n",
      "Epoch 75/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0158Epoch 75/100 - loss: 0.0158 - val_loss: 0.0160\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0158 - val_loss: 0.0160\n",
      "Epoch 76/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0156Epoch 76/100 - loss: 0.0156 - val_loss: 0.0156\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/138 [============================>.] - ETA: 0s - loss: 0.0154Epoch 77/100 - loss: 0.0154 - val_loss: 0.0158\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0154 - val_loss: 0.0158\n",
      "Epoch 78/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0155Epoch 78/100 - loss: 0.0155 - val_loss: 0.0151\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0155 - val_loss: 0.0151\n",
      "Epoch 79/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0154Epoch 79/100 - loss: 0.0154 - val_loss: 0.0151\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0154 - val_loss: 0.0151\n",
      "Epoch 80/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0155Epoch 80/100 - loss: 0.0155 - val_loss: 0.0156\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0155 - val_loss: 0.0156\n",
      "Epoch 81/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0155Epoch 81/100 - loss: 0.0156 - val_loss: 0.0162\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0156 - val_loss: 0.0162\n",
      "Epoch 82/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0154Epoch 82/100 - loss: 0.0154 - val_loss: 0.0152\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0154 - val_loss: 0.0152\n",
      "Epoch 83/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0154Epoch 83/100 - loss: 0.0154 - val_loss: 0.0150\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0154 - val_loss: 0.0150\n",
      "Epoch 84/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0153Epoch 84/100 - loss: 0.0153 - val_loss: 0.0151\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0153 - val_loss: 0.0151\n",
      "Epoch 85/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0153Epoch 85/100 - loss: 0.0153 - val_loss: 0.0152\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0153 - val_loss: 0.0152\n",
      "Epoch 86/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0153Epoch 86/100 - loss: 0.0153 - val_loss: 0.0148\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0153 - val_loss: 0.0148\n",
      "Epoch 87/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0152Epoch 87/100 - loss: 0.0152 - val_loss: 0.0150\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0152 - val_loss: 0.0150\n",
      "Epoch 88/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0151Epoch 88/100 - loss: 0.0151 - val_loss: 0.0148\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0151 - val_loss: 0.0148\n",
      "Epoch 89/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0151Epoch 89/100 - loss: 0.0151 - val_loss: 0.0147\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0151 - val_loss: 0.0147\n",
      "Epoch 90/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0151Epoch 90/100 - loss: 0.0151 - val_loss: 0.0147\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0151 - val_loss: 0.0147\n",
      "Epoch 91/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 91/100 - loss: 0.0150 - val_loss: 0.0146\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "Epoch 92/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 92/100 - loss: 0.0151 - val_loss: 0.0148\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0151 - val_loss: 0.0148\n",
      "Epoch 93/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 93/100 - loss: 0.0150 - val_loss: 0.0146\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "Epoch 94/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 94/100 - loss: 0.0150 - val_loss: 0.0145\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0150 - val_loss: 0.0145\n",
      "Epoch 95/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0149Epoch 95/100 - loss: 0.0149 - val_loss: 0.0145\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0149 - val_loss: 0.0145\n",
      "Epoch 96/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0149Epoch 96/100 - loss: 0.0149 - val_loss: 0.0145\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0149 - val_loss: 0.0145\n",
      "Epoch 97/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0149Epoch 97/100 - loss: 0.0149 - val_loss: 0.0144\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0149 - val_loss: 0.0144\n",
      "Epoch 98/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 98/100 - loss: 0.0150 - val_loss: 0.0144\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0150 - val_loss: 0.0144\n",
      "Epoch 99/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0149Epoch 99/100 - loss: 0.0149 - val_loss: 0.0144\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0149 - val_loss: 0.0144\n",
      "Epoch 100/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 100/100 - loss: 0.0149 - val_loss: 0.0144\n",
      "138/138 [==============================] - 3s 21ms/step - loss: 0.0149 - val_loss: 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/20 18:14:41 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpihufc10w/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpihufc10w/model/data/model/assets\n",
      "2024/08/20 18:14:46 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpihufc10w/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/08/20 18:14:46 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step\n",
      "\n",
      "Model__e100_BN_1_relu_relu_relu_relu_lr_0.04 Model:\n",
      "Mean Error: 0.0247\n",
      "Median Error: 0.0265\n",
      "90th Percentile Error: 0.0423\n",
      "Max Error: 3.3450\n",
      "Training Time: 291.40 seconds\n",
      "\n",
      "Model__e100_BN_1_relu_relu_relu_relu_lr_0.01:\n",
      "Mean Error: 0.0259\n",
      "Median Error: 0.0263\n",
      "90th Percentile Error: 0.0436\n",
      "Max Error: 3.8259\n",
      "Training Time: 294.08 seconds\n",
      "\n",
      "Model__e100_BN_1_relu_relu_relu_relu_lr_0.04:\n",
      "Mean Error: 0.0247\n",
      "Median Error: 0.0265\n",
      "90th Percentile Error: 0.0423\n",
      "Max Error: 3.3450\n",
      "Training Time: 291.40 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run experiments\n",
    "results = {}\n",
    "for config in configs:\n",
    "    print(f\"Running experiment: {config['model_name']}\")\n",
    "    results[config['model_name']] = run_single_experiment(config)\n",
    "\n",
    "# Print summary of results\n",
    "for model_name, result in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Mean Error: {np.mean(result['errors']):.4f}\")\n",
    "    print(f\"Median Error: {np.median(result['errors']):.4f}\")\n",
    "    print(f\"90th Percentile Error: {np.percentile(result['errors'], 90):.4f}\")\n",
    "    print(f\"Max Error: {np.max(result['errors']):.4f}\")\n",
    "    print(f\"Training Time: {result['training_time']:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
