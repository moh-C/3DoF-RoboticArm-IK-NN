{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(fk_weight, fk_loss_type, exp_scale=1.0, huber_delta=0.01):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Mean Squared Error for joint angles\n",
    "        angle_loss = tf.keras.losses.Huber(delta=huber_delta)(y_true, y_pred)\n",
    "        \n",
    "        # Forward kinematics loss\n",
    "        fk_true = forward_kinematics_tf(y_true)\n",
    "        fk_pred = forward_kinematics_tf(y_pred)\n",
    "        \n",
    "        if fk_loss_type == 'exponential':\n",
    "            fk_diff = tf.reduce_sum(tf.square(fk_true - fk_pred), axis=-1)\n",
    "            fk_loss = tf.reduce_mean(1 - tf.exp(-exp_scale * fk_diff))\n",
    "        elif fk_loss_type == 'huber':\n",
    "            fk_loss = tf.keras.losses.Huber(delta=huber_delta)(fk_true, fk_pred)\n",
    "        elif fk_loss_type == 'linear':\n",
    "            fk_loss = tf.reduce_mean(tf.abs(fk_true - fk_pred))\n",
    "        elif fk_loss_type == 'mae':\n",
    "            fk_loss = tf.keras.losses.mae(fk_true, fk_pred)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported forward kinematics loss type: {fk_loss_type}\")\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = angle_loss + fk_weight * fk_loss\n",
    "        return total_loss\n",
    "    return loss_fn\n",
    "\n",
    "def create_model(config):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(3,)),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(256),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(256),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(128),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(3, activation='tanh')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=custom_loss(\n",
    "        config['fk_weight'], \n",
    "        config['fk_loss_type'],\n",
    "        exp_scale=config.get('exp_scale', 1.0),\n",
    "        huber_delta=config.get('huber_delta', 1.0)\n",
    "    ))\n",
    "    return model\n",
    "\n",
    "def run_single_experiment(config):\n",
    "    (train_inputs, train_outputs), (test_inputs, test_outputs), input_mean, input_std = load_and_preprocess_data()\n",
    "    \n",
    "    valid_inputs, test_inputs, valid_outputs, test_outputs = train_test_split(\n",
    "        test_inputs, test_outputs, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    mlflow.set_experiment(config['experiment_name'])\n",
    "    \n",
    "    with mlflow.start_run(run_name=config['model_name']):\n",
    "        mlflow.log_params(config)\n",
    "        \n",
    "        steps_per_epoch = len(train_inputs) // config['batch_size']\n",
    "        total_steps = steps_per_epoch * config['epochs']\n",
    "        warmup_steps = int(0.1 * total_steps)\n",
    "        \n",
    "        model = create_model(config)\n",
    "        \n",
    "        model_summary = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: model_summary.write(x + '\\n'))\n",
    "        mlflow.log_text(model_summary.getvalue(), \"model_summary.txt\")\n",
    "        \n",
    "        lr_scheduler = CosineDecayWithWarmupCallback(\n",
    "            config['initial_learning_rate'],\n",
    "            warmup_steps,\n",
    "            total_steps\n",
    "        )\n",
    "        verbose_logging = VerboseLoggingCallback()\n",
    "        lr_logger = LearningRateLogger()\n",
    "        \n",
    "        callbacks = [lr_scheduler, verbose_logging, lr_logger]\n",
    "        \n",
    "        callback_names = [callback.__class__.__name__ for callback in callbacks]\n",
    "        mlflow.log_param(\"callbacks\", \", \".join(callback_names))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            train_inputs, train_outputs,\n",
    "            epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            validation_data=(valid_inputs, valid_outputs),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        for epoch, (loss, val_loss) in enumerate(zip(history.history['loss'], history.history['val_loss'])):\n",
    "            mlflow.log_metric(\"train_loss\", loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        \n",
    "        mlflow.log_metric(\"training_time\", training_time)\n",
    "        \n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        \n",
    "        mlflow.log_text(verbose_logging.get_output(), \"training_output.txt\")\n",
    "        \n",
    "        errors, true_xyz, predicted_xyz = evaluate_model(model, test_inputs, test_outputs, input_mean, input_std)\n",
    "        \n",
    "        mlflow.log_metric(\"mean_error\", np.mean(errors))\n",
    "        mlflow.log_metric(\"median_error\", np.median(errors))\n",
    "        mlflow.log_metric(\"90th_percentile_error\", np.percentile(errors, 90))\n",
    "        mlflow.log_metric(\"max_error\", np.max(errors))\n",
    "        \n",
    "        true_vs_pred_plot_path = f\"./Figures/Regularization/{config['model_name']}_true_vs_predicted.png\"\n",
    "        plot_true_vs_predicted(true_xyz, predicted_xyz, f\"{config['model_name']} Model: True vs Predicted\", save_path=true_vs_pred_plot_path)\n",
    "        mlflow.log_artifact(true_vs_pred_plot_path)\n",
    "        \n",
    "        error_dist_plot_path = f\"./Figures/Regularization/{config['model_name']}_error_distribution.png\"\n",
    "        plot_error_distribution(errors, f\"{config['model_name']} Model: Error Distribution\", save_path=error_dist_plot_path)\n",
    "        mlflow.log_artifact(error_dist_plot_path)\n",
    "        \n",
    "        print(f\"\\n{config['model_name']} Model:\")\n",
    "        print(f\"Mean Error: {np.mean(errors):.4f}\")\n",
    "        print(f\"Median Error: {np.median(errors):.4f}\")\n",
    "        print(f\"90th Percentile Error: {np.percentile(errors, 90):.4f}\")\n",
    "        print(f\"Max Error: {np.max(errors):.4f}\")\n",
    "        print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'errors': errors,\n",
    "            'true_xyz': true_xyz,\n",
    "            'predicted_xyz': predicted_xyz,\n",
    "            'training_time': training_time\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configurations\n",
    "configs = []\n",
    "fk_weights = [10, 20]\n",
    "fk_loss_types = ['exponential', 'huber', 'linear', 'mae']\n",
    "learning_rates = [0.001, 0.01, 0.05]\n",
    "\n",
    "# Parameters specific to certain loss types\n",
    "exp_scales = [0.1, 0.5, 2.0]\n",
    "huber_deltas = [0.01, 0.05]\n",
    "\n",
    "for fk_weight in fk_weights:\n",
    "    for fk_loss_type in fk_loss_types:\n",
    "        for learning_rate in learning_rates:\n",
    "            if fk_loss_type == 'exponential':\n",
    "                for exp_scale in exp_scales:\n",
    "                    config = {\n",
    "                        \"model_name\": f\"Model_FKWeight_{fk_weight}_ExpLoss_Scale_{exp_scale}_LR_{learning_rate}\",\n",
    "                        \"fk_weight\": fk_weight,\n",
    "                        \"fk_loss_type\": fk_loss_type,\n",
    "                        \"exp_scale\": exp_scale,\n",
    "                        \"epochs\": 100,\n",
    "                        \"initial_learning_rate\": learning_rate,\n",
    "                        \"batch_size\": 65536*2,\n",
    "                        \"experiment_name\": \"Inverse Kinematics Multi-Loss\"\n",
    "                    }\n",
    "                    configs.append(config)\n",
    "            elif fk_loss_type == 'huber':\n",
    "                for huber_delta in huber_deltas:\n",
    "                    config = {\n",
    "                        \"model_name\": f\"Model_FKWeight_{fk_weight}_HuberLoss_Delta_{huber_delta}_LR_{learning_rate}\",\n",
    "                        \"fk_weight\": fk_weight,\n",
    "                        \"fk_loss_type\": fk_loss_type,\n",
    "                        \"huber_delta\": huber_delta,\n",
    "                        \"epochs\": 100,\n",
    "                        \"initial_learning_rate\": learning_rate,\n",
    "                        \"batch_size\": 65536*2,\n",
    "                        \"experiment_name\": \"Inverse Kinematics Multi-Loss\"\n",
    "                    }\n",
    "                    configs.append(config)\n",
    "            else:  # linear and mae\n",
    "                config = {\n",
    "                    \"model_name\": f\"Model_FKWeight_{fk_weight}_{fk_loss_type.capitalize()}Loss_LR_{learning_rate}\",\n",
    "                    \"fk_weight\": fk_weight,\n",
    "                    \"fk_loss_type\": fk_loss_type,\n",
    "                    \"epochs\": 100,\n",
    "                    \"initial_learning_rate\": learning_rate,\n",
    "                    \"batch_size\": 65536*2,\n",
    "                    \"experiment_name\": \"Inverse Kinematics Multi-Loss\"\n",
    "                }\n",
    "                configs.append(config)\n",
    "\n",
    "# Print the total number of configurations\n",
    "print(f\"Total number of configurations: {len(configs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "for config in configs:\n",
    "    run_single_experiment(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
