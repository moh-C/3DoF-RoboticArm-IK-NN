{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import os\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "L1, L2, L3 = 1.0, 1.5, 0.5  # link lengths\n",
    "\n",
    "@tf.function\n",
    "def forward_kinematics_tf(theta):\n",
    "    theta1, theta2, theta3 = tf.unstack(theta, axis=1)\n",
    "    \n",
    "    x = L1 * tf.cos(theta1) * tf.sin(theta2) + L2 * tf.cos(theta1) * tf.sin(theta2 + theta3)\n",
    "    y = L1 * tf.sin(theta1) * tf.sin(theta2) + L2 * tf.sin(theta1) * tf.sin(theta2 + theta3)\n",
    "    z = L1 * tf.cos(theta2) + L2 * tf.cos(theta2 + theta3) + L3\n",
    "    \n",
    "    return tf.stack([x, y, z], axis=1)\n",
    "\n",
    "def evaluate_model(model, test_inputs, test_outputs, input_mean, input_std, batch_size=2**16):\n",
    "    test_inputs = tf.convert_to_tensor(test_inputs, dtype=tf.float32)\n",
    "    input_mean = tf.convert_to_tensor(input_mean, dtype=tf.float32)\n",
    "    input_std = tf.convert_to_tensor(input_std, dtype=tf.float32)\n",
    "    predicted_angles_normalized = model.predict(test_inputs, batch_size=batch_size)\n",
    "    predicted_angles = predicted_angles_normalized * (np.pi/2)\n",
    "    true_xyz = test_inputs * input_std + input_mean\n",
    "    predicted_xyz = forward_kinematics_tf(predicted_angles)\n",
    "    errors = tf.norm(true_xyz - predicted_xyz, axis=1)\n",
    "    return errors.numpy(), true_xyz.numpy(), predicted_xyz.numpy()\n",
    "\n",
    "def custom_loss(fk_weight=1, delta=0.2):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Huber loss for joint angles\n",
    "        angle_loss = huber_loss(y_true, y_pred, delta)\n",
    "        \n",
    "        # Forward kinematics loss (using MSE or another appropriate metric)\n",
    "        fk_true = forward_kinematics_tf(y_true)\n",
    "        fk_pred = forward_kinematics_tf(y_pred)\n",
    "        fk_loss = tf.reduce_mean(tf.square(fk_true - fk_pred))\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = tf.reduce_mean(angle_loss) + fk_weight * fk_loss\n",
    "        return total_loss\n",
    "    return loss_fn\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) <= delta\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = delta * (tf.abs(error) - delta / 2)\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "def create_model(config):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, activation='relu', input_shape=(3,)),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(3, activation='tanh')\n",
    "    ])\n",
    "    \n",
    "    loss_type = config['loss_type']\n",
    "    if loss_type == 'standard':\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "    elif loss_type == 'custom':\n",
    "        model.compile(optimizer='adam', loss=custom_loss())\n",
    "    elif loss_type == 'modified_custom':\n",
    "        model.compile(optimizer='adam', loss=custom_loss(fk_weight=config.get('fk_weight', 10)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_and_preprocess_data(filename='robot_arm_dataset_10M.npz'):\n",
    "    data = np.load(f'./Data/{filename}')\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    \n",
    "    input_mean = np.mean(inputs, axis=0)\n",
    "    input_std = np.std(inputs, axis=0)\n",
    "    inputs_normalized = (inputs - input_mean) / input_std\n",
    "\n",
    "    outputs_normalized = outputs / (np.pi/2)\n",
    "\n",
    "    split_index = int(0.9 * len(inputs))\n",
    "    train_inputs, test_inputs = inputs_normalized[:split_index], inputs_normalized[split_index:]\n",
    "    train_outputs, test_outputs = outputs_normalized[:split_index], outputs_normalized[split_index:]\n",
    "\n",
    "    return (train_inputs, train_outputs), (test_inputs, test_outputs), input_mean, input_std\n",
    "\n",
    "class VerboseLoggingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = io.StringIO()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        output = f\"Epoch {epoch+1}/{self.params['epochs']} - \"\n",
    "        output += \" - \".join(f\"{k}: {v:.4f}\" for k, v in logs.items())\n",
    "        print(output)\n",
    "        self.output.write(output + \"\\n\")\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.output.getvalue()\n",
    "\n",
    "class LearningRateLogger(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'value'):\n",
    "            lr = lr.value()\n",
    "        mlflow.log_metric(\"learning_rate\", lr, step=epoch)\n",
    "        \n",
    "class CosineDecayWithWarmupCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_learning_rate, warmup_steps, total_steps):\n",
    "        super(CosineDecayWithWarmupCallback, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            lr = self.initial_learning_rate * (self.current_step / self.warmup_steps)\n",
    "        else:\n",
    "            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            lr = 0.5 * self.initial_learning_rate * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        self.current_step += 1\n",
    "\n",
    "def plot_error_distribution(errors, title, save_path=None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(errors, bins=100, alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Error (Euclidean distance)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlim([0, 1.5])\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_true_vs_predicted(true_xyz, predicted_xyz, title, save_path=None):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    for i, coord in enumerate(['X', 'Y', 'Z']):\n",
    "        ax = axes[i]\n",
    "        ax.scatter(true_xyz[:, i], predicted_xyz[:, i], alpha=0.1)\n",
    "        ax.plot([true_xyz[:, i].min(), true_xyz[:, i].max()], [true_xyz[:, i].min(), true_xyz[:, i].max()], 'r--')\n",
    "        ax.set_xlabel(f'True {coord}')\n",
    "        ax.set_ylabel(f'Predicted {coord}')\n",
    "        ax.set_title(f'{coord} Coordinate: True vs Predicted')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "def train_and_evaluate_model(train_data, valid_data, test_data, input_mean, input_std, config):\n",
    "    train_inputs, train_outputs = train_data\n",
    "    valid_inputs, valid_outputs = valid_data\n",
    "    test_inputs, test_outputs = test_data\n",
    "    \n",
    "    experiment_name = config['experiment_name']\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=config['model_name']):\n",
    "        # Calculate total steps\n",
    "        steps_per_epoch = len(train_inputs) // config['batch_size']\n",
    "        total_steps = steps_per_epoch * epochs\n",
    "        warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(config)\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = create_model(config)\n",
    "        \n",
    "        # Log model summary\n",
    "        model_summary = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: model_summary.write(x + '\\n'))\n",
    "        mlflow.log_text(model_summary.getvalue(), \"model_summary.txt\")\n",
    "        \n",
    "        # Set up callbacks\n",
    "        early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        \n",
    "        lr_scheduler = CosineDecayWithWarmupCallback(\n",
    "            config['initial_learning_rate'],\n",
    "            warmup_steps,\n",
    "            total_steps\n",
    "        )\n",
    "        \n",
    "        verbose_logging = VerboseLoggingCallback()\n",
    "        lr_logger = LearningRateLogger()\n",
    "        \n",
    "        callbacks = [early_stopping, lr_scheduler, verbose_logging, lr_logger]\n",
    "        \n",
    "        # Log callback names\n",
    "        callback_names = [callback.__class__.__name__ for callback in callbacks]\n",
    "        mlflow.log_param(\"callbacks\", \", \".join(callback_names))\n",
    "        \n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            train_inputs, train_outputs,\n",
    "            epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            validation_data=(valid_inputs, valid_outputs),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Log training metrics\n",
    "        for epoch, (loss, val_loss) in enumerate(zip(history.history['loss'], history.history['val_loss'])):\n",
    "            mlflow.log_metric(\"train_loss\", loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        \n",
    "        mlflow.log_metric(\"training_time\", training_time)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        \n",
    "        # Log training output\n",
    "        mlflow.log_text(verbose_logging.get_output(), \"training_output.txt\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        errors, true_xyz, predicted_xyz = evaluate_model(model, test_inputs, test_outputs, input_mean, input_std)\n",
    "        \n",
    "        # Log evaluation metrics\n",
    "        mlflow.log_metric(\"mean_error\", np.mean(errors))\n",
    "        mlflow.log_metric(\"median_error\", np.median(errors))\n",
    "        mlflow.log_metric(\"90th_percentile_error\", np.percentile(errors, 90))\n",
    "        mlflow.log_metric(\"max_error\", np.max(errors))\n",
    "        \n",
    "        # Generate and log the true vs predicted plot\n",
    "        true_vs_pred_plot_path = f\"{config['model_name']}_true_vs_predicted.png\"\n",
    "        plot_true_vs_predicted(true_xyz, predicted_xyz, f\"{config['model_name']} Model: True vs Predicted\", save_path=true_vs_pred_plot_path)\n",
    "        mlflow.log_artifact(true_vs_pred_plot_path)\n",
    "        os.remove(true_vs_pred_plot_path)  # Clean up the temporary file\n",
    "        \n",
    "        # Generate and log the error distribution plot\n",
    "        error_dist_plot_path = f\"{config['model_name']}_error_distribution.png\"\n",
    "        plot_error_distribution(errors, f\"{config['model_name']} Model: Error Distribution\", save_path=error_dist_plot_path)\n",
    "        mlflow.log_artifact(error_dist_plot_path)\n",
    "        os.remove(error_dist_plot_path)  # Clean up the temporary file\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\n{config['model_name']} Model:\")\n",
    "        print(f\"Mean Error: {np.mean(errors):.4f}\")\n",
    "        print(f\"Median Error: {np.median(errors):.4f}\")\n",
    "        print(f\"90th Percentile Error: {np.percentile(errors, 90):.4f}\")\n",
    "        print(f\"Max Error: {np.max(errors):.4f}\")\n",
    "        print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'errors': errors,\n",
    "            'true_xyz': true_xyz,\n",
    "            'predicted_xyz': predicted_xyz,\n",
    "            'training_time': training_time\n",
    "        }\n",
    "\n",
    "def run_single_experiment(config):\n",
    "    (train_inputs, train_outputs), (test_inputs, test_outputs), input_mean, input_std = load_and_preprocess_data()\n",
    "    \n",
    "    # Split test data into validation and test sets\n",
    "    valid_inputs, test_inputs, valid_outputs, test_outputs = train_test_split(\n",
    "        test_inputs, test_outputs, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_and_evaluate_model(\n",
    "        (train_inputs, train_outputs),\n",
    "        (valid_inputs, valid_outputs),\n",
    "        (test_inputs, test_outputs),\n",
    "        input_mean, input_std,\n",
    "        config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 317, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 410, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1341, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1334, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/utils/file_utils.py\", line 309, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.1794Epoch 1/10 - loss: 0.1783 - val_loss: 0.1125\n",
      "138/138 [==============================] - 3s 10ms/step - loss: 0.1783 - val_loss: 0.1125\n",
      "Epoch 2/10\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.1041Epoch 2/10 - loss: 0.1038 - val_loss: 0.0936\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.1038 - val_loss: 0.0936\n",
      "Epoch 3/10\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0898Epoch 3/10 - loss: 0.0898 - val_loss: 0.0878\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0898 - val_loss: 0.0878\n",
      "Epoch 4/10\n",
      "132/138 [===========================>..] - ETA: 0s - loss: 0.0872Epoch 4/10 - loss: 0.0871 - val_loss: 0.0868\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0871 - val_loss: 0.0868\n",
      "Epoch 5/10\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0863Epoch 5/10 - loss: 0.0863 - val_loss: 0.0860\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0863 - val_loss: 0.0860\n",
      "Epoch 6/10\n",
      "132/138 [===========================>..] - ETA: 0s - loss: 0.0858Epoch 6/10 - loss: 0.0858 - val_loss: 0.0857\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0858 - val_loss: 0.0857\n",
      "Epoch 7/10\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0855Epoch 7/10 - loss: 0.0855 - val_loss: 0.0855\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0855 - val_loss: 0.0855\n",
      "Epoch 8/10\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0853Epoch 8/10 - loss: 0.0853 - val_loss: 0.0853\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0853 - val_loss: 0.0853\n",
      "Epoch 9/10\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0852Epoch 9/10 - loss: 0.0852 - val_loss: 0.0852\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0852 - val_loss: 0.0852\n",
      "Epoch 10/10\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0852Epoch 10/10 - loss: 0.0852 - val_loss: 0.0852\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0852 - val_loss: 0.0852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/19 23:14:36 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpadwtd8kv/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpadwtd8kv/model/data/model/assets\n",
      "/usr/local/lib/python3.8/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2024/08/19 23:14:41 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n",
      "\n",
      "Huber Loss Joint Model:\n",
      "Mean Error: 0.2026\n",
      "Median Error: 0.1515\n",
      "90th Percentile Error: 0.4503\n",
      "Max Error: 4.1185\n",
      "Training Time: 13.93 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 317, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 410, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1341, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1334, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/utils/file_utils.py\", line 309, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 2.1098Epoch 1/10 - loss: 2.0529 - val_loss: 0.2840\n",
      "138/138 [==============================] - 2s 10ms/step - loss: 2.0529 - val_loss: 0.2840\n",
      "Epoch 2/10\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.1805Epoch 2/10 - loss: 0.1794 - val_loss: 0.1359\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.1794 - val_loss: 0.1359\n",
      "Epoch 3/10\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.1221Epoch 3/10 - loss: 0.1219 - val_loss: 0.1115\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.1219 - val_loss: 0.1115\n",
      "Epoch 4/10\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.1042Epoch 4/10 - loss: 0.1040 - val_loss: 0.0995\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.1040 - val_loss: 0.0995\n",
      "Epoch 5/10\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0926Epoch 5/10 - loss: 0.0924 - val_loss: 0.0886\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0924 - val_loss: 0.0886\n",
      "Epoch 6/10\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0843Epoch 6/10 - loss: 0.0843 - val_loss: 0.0811\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0843 - val_loss: 0.0811\n",
      "Epoch 7/10\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0762Epoch 7/10 - loss: 0.0761 - val_loss: 0.0725\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0761 - val_loss: 0.0725\n",
      "Epoch 8/10\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0684Epoch 8/10 - loss: 0.0684 - val_loss: 0.0663\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0684 - val_loss: 0.0663\n",
      "Epoch 9/10\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0639Epoch 9/10 - loss: 0.0639 - val_loss: 0.0639\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0639 - val_loss: 0.0639\n",
      "Epoch 10/10\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0627Epoch 10/10 - loss: 0.0627 - val_loss: 0.0636\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0627 - val_loss: 0.0636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/19 23:15:01 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp3xrvtkim/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp3xrvtkim/model/data/model/assets\n",
      "2024/08/19 23:15:05 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp3xrvtkim/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/08/19 23:15:05 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n",
      "\n",
      "Huber Loss Joint, fk=5 Model:\n",
      "Mean Error: 0.1305\n",
      "Median Error: 0.0952\n",
      "90th Percentile Error: 0.2682\n",
      "Max Error: 4.0647\n",
      "Training Time: 14.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 317, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 410, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1341, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1334, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/utils/file_utils.py\", line 309, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "137/138 [============================>.] - ETA: 0s - loss: 2.3614Epoch 1/10 - loss: 2.3565 - val_loss: 0.2742\n",
      "138/138 [==============================] - 2s 10ms/step - loss: 2.3565 - val_loss: 0.2742\n",
      "Epoch 2/10\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.1798Epoch 2/10 - loss: 0.1788 - val_loss: 0.1375\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.1788 - val_loss: 0.1375\n",
      "Epoch 3/10\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.1186Epoch 3/10 - loss: 0.1183 - val_loss: 0.1042\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.1183 - val_loss: 0.1042\n",
      "Epoch 4/10\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0921Epoch 4/10 - loss: 0.0921 - val_loss: 0.0812\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0921 - val_loss: 0.0812\n",
      "Epoch 5/10\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0731Epoch 5/10 - loss: 0.0731 - val_loss: 0.0683\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0731 - val_loss: 0.0683\n",
      "Epoch 6/10\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0662Epoch 6/10 - loss: 0.0661 - val_loss: 0.0649\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0661 - val_loss: 0.0649\n",
      "Epoch 7/10\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0618Epoch 7/10 - loss: 0.0618 - val_loss: 0.0612\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0618 - val_loss: 0.0612\n",
      "Epoch 8/10\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0599Epoch 8/10 - loss: 0.0598 - val_loss: 0.0599\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0598 - val_loss: 0.0599\n",
      "Epoch 9/10\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0584Epoch 9/10 - loss: 0.0584 - val_loss: 0.0590\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0584 - val_loss: 0.0590\n",
      "Epoch 10/10\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0581Epoch 10/10 - loss: 0.0580 - val_loss: 0.0588\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0580 - val_loss: 0.0588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/19 23:15:26 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpttodsjv6/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpttodsjv6/model/data/model/assets\n",
      "2024/08/19 23:15:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpttodsjv6/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/08/19 23:15:30 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n",
      "\n",
      "Huber Loss joint, fk=10 Model:\n",
      "Mean Error: 0.1074\n",
      "Median Error: 0.0754\n",
      "90th Percentile Error: 0.2136\n",
      "Max Error: 4.1159\n",
      "Training Time: 14.04 seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "configs = [\n",
    "  {\n",
    "    \"model_name\": \"Huber Loss Joint, fk=5\",\n",
    "    \"loss_type\": \"modified_custom\",\n",
    "    \"fk_weight\": 10,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": 65536,\n",
    "    \"initial_learning_rate\": 1e-3,\n",
    "    \"experiment_name\": \"Inverse Kinematics NN Comparison\"\n",
    "  },\n",
    "    {\n",
    "    \"model_name\": \"Huber Loss joint, fk=10\",\n",
    "    \"loss_type\": \"modified_custom\",\n",
    "    \"fk_weight\": 10,\n",
    "    \"epochs\": epochs,\n",
    "    \"initial_learning_rate\": 1e-3,\n",
    "    \"batch_size\": 65536,\n",
    "    \"experiment_name\": \"Inverse Kinematics NN Comparison\"\n",
    "  }\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for config in configs:\n",
    "    results[config['model_name']] = run_single_experiment(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
