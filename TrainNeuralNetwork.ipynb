{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io, os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_dataset(filename='robot_arm_dataset_10M.npz'):\n",
    "    data = np.load(f'./Data/{filename}')\n",
    "    return data['inputs'], data['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_model(input_shape, output_shape):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        keras.layers.Dense(128),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(64),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.Dense(output_shape)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseLoggingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = io.StringIO()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        output = f\"Epoch {epoch+1}/{self.params['epochs']} - \"\n",
    "        output += \" - \".join(f\"{k}: {v:.4f}\" for k, v in logs.items())\n",
    "        print(output)\n",
    "        self.output.write(output + \"\\n\")\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.output.getvalue()\n",
    "    \n",
    "class LearningRateLogger(keras.callbacks.Callback):\n",
    "    def __init__(self, tensorboard_writer):\n",
    "        super().__init__()\n",
    "        self.tensorboard_writer = tensorboard_writer\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'value'):\n",
    "            lr = lr.value()\n",
    "        with self.tensorboard_writer.as_default():\n",
    "            tf.summary.scalar('learning_rate', data=lr, step=epoch)\n",
    "        mlflow.log_metric(\"learning_rate\", lr, step=epoch)\n",
    "        \n",
    "class CosineDecayWithWarmupCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_learning_rate, warmup_steps, total_steps):\n",
    "        super(CosineDecayWithWarmupCallback, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            lr = self.initial_learning_rate * (self.current_step / self.warmup_steps)\n",
    "        else:\n",
    "            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            lr = 0.5 * self.initial_learning_rate * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        self.current_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(batch_size, epochs, initial_learning_rate, test_size=0.2, experiment_name=\"Inverse Kinematics NN\", run_name=None):\n",
    "    # Set up MLflow\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    # Generate a unique run name if one is provided\n",
    "    if run_name:\n",
    "        timestamp = int(time.time())\n",
    "        unique_run_name = f\"{run_name}_{timestamp}\"\n",
    "    else:\n",
    "        unique_run_name = None\n",
    "        \n",
    "    with mlflow.start_run(run_name=unique_run_name) as run:\n",
    "        # Create a consistent directory structure for TensorBoard logs\n",
    "        run_id = run.info.run_id\n",
    "        run_name = run.data.tags.get('mlflow.runName', run_id)\n",
    "        log_dir = os.path.join(\"logs\", experiment_name, unique_run_name)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        \n",
    "        # Load and split the data\n",
    "        X, y = load_dataset()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"initial_learning_rate\", initial_learning_rate)\n",
    "        mlflow.log_param(\"test_size\", test_size)\n",
    "\n",
    "        # Calculate total steps\n",
    "        steps_per_epoch = len(X_train) // batch_size\n",
    "        total_steps = steps_per_epoch * epochs\n",
    "        warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n",
    "        \n",
    "        # Create and compile the model\n",
    "        model = create_model(input_shape=(3,), output_shape=3)\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Log model summary\n",
    "        model_summary = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: model_summary.write(x + '\\n'))\n",
    "        mlflow.log_text(model_summary.getvalue(), \"model_summary.txt\")\n",
    "\n",
    "        # Set up TensorBoard callback and writer\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        tensorboard_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "        # Set up other callbacks\n",
    "        early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        lr_scheduler = CosineDecayWithWarmupCallback(initial_learning_rate, warmup_steps, total_steps)\n",
    "        lr_logger = LearningRateLogger(tensorboard_writer)\n",
    "        \n",
    "        callbacks = [tensorboard_callback, lr_scheduler, lr_logger]\n",
    "\n",
    "        # Log callback names\n",
    "        callback_names = [callback.__class__.__name__ for callback in callbacks]\n",
    "        mlflow.log_param(\"callbacks\", \", \".join(callback_names))\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Log metrics\n",
    "        for epoch, (loss, val_loss) in enumerate(zip(\n",
    "            history.history['loss'],\n",
    "            history.history['val_loss']\n",
    "        )):\n",
    "            mlflow.log_metric(\"train_loss\", loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "        # Log the TensorBoard log directory\n",
    "        mlflow.log_param(\"tensorboard_log_dir\", log_dir)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "\n",
    "    print(\"Training completed and logged with MLflow and TensorBoard.\")\n",
    "    print(f\"Experiment name: {experiment_name}\")\n",
    "    print(f\"Run name: {run_name}\")\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"TensorBoard logs saved to: {log_dir}\")\n",
    "    print(\"To view in TensorBoard, run:\")\n",
    "    print(f\"tensorboard --logdir logs/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "  1/245 [..............................] - ETA: 3:27 - loss: 2.7286WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_end` time: 0.0064s). Check your callbacks.\n",
      "245/245 [==============================] - 3s 9ms/step - loss: 0.5275 - val_loss: 0.6850\n",
      "Epoch 2/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3345 - val_loss: 0.5021\n",
      "Epoch 3/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3009 - val_loss: 0.3995\n",
      "Epoch 4/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2907 - val_loss: 0.3492\n",
      "Epoch 5/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2763 - val_loss: 0.2993\n",
      "Epoch 6/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2769 - val_loss: 0.3164\n",
      "Epoch 7/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2740 - val_loss: 0.3765\n",
      "Epoch 8/75\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2694 - val_loss: 0.2801\n",
      "Epoch 9/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2633 - val_loss: 0.3014\n",
      "Epoch 10/75\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2629 - val_loss: 0.7399\n",
      "Epoch 11/75\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2582 - val_loss: 0.2670\n",
      "Epoch 12/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2562 - val_loss: 0.2708\n",
      "Epoch 13/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2537 - val_loss: 0.2542\n",
      "Epoch 14/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2554 - val_loss: 0.3211\n",
      "Epoch 15/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2498 - val_loss: 0.2676\n",
      "Epoch 16/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2490 - val_loss: 0.2621\n",
      "Epoch 17/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2477 - val_loss: 0.2615\n",
      "Epoch 18/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2459 - val_loss: 0.2443\n",
      "Epoch 19/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2456 - val_loss: 0.2790\n",
      "Epoch 20/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2428 - val_loss: 0.2408\n",
      "Epoch 21/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2422 - val_loss: 0.2542\n",
      "Epoch 22/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2420 - val_loss: 0.2640\n",
      "Epoch 23/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2409 - val_loss: 0.2559\n",
      "Epoch 24/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2404 - val_loss: 0.2461\n",
      "Epoch 25/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2390 - val_loss: 0.2667\n",
      "Epoch 26/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2393 - val_loss: 0.2416\n",
      "Epoch 27/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2358 - val_loss: 0.2834\n",
      "Epoch 28/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2372 - val_loss: 0.2549\n",
      "Epoch 29/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2358 - val_loss: 0.2555\n",
      "Epoch 30/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2348 - val_loss: 0.2498\n",
      "Epoch 31/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2337 - val_loss: 0.2430\n",
      "Epoch 32/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2343 - val_loss: 0.2514\n",
      "Epoch 33/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2331 - val_loss: 0.2403\n",
      "Epoch 34/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2321 - val_loss: 0.2359\n",
      "Epoch 35/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2331 - val_loss: 0.2429\n",
      "Epoch 36/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2310 - val_loss: 0.2394\n",
      "Epoch 37/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2297 - val_loss: 0.2330\n",
      "Epoch 38/75\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2299 - val_loss: 0.2384\n",
      "Epoch 39/75\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2298 - val_loss: 0.2317\n",
      "Epoch 40/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2297 - val_loss: 0.2434\n",
      "Epoch 41/75\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2280 - val_loss: 0.2380\n",
      "Epoch 42/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2275 - val_loss: 0.2501\n",
      "Epoch 43/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2272 - val_loss: 0.2383\n",
      "Epoch 44/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2266 - val_loss: 0.2400\n",
      "Epoch 45/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2265 - val_loss: 0.2277\n",
      "Epoch 46/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2246 - val_loss: 0.2838\n",
      "Epoch 47/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2259 - val_loss: 0.2226\n",
      "Epoch 48/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2240 - val_loss: 0.2238\n",
      "Epoch 49/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2241 - val_loss: 0.2326\n",
      "Epoch 50/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2234 - val_loss: 0.2295\n",
      "Epoch 51/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2234 - val_loss: 0.2380\n",
      "Epoch 52/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2229 - val_loss: 0.2228\n",
      "Epoch 53/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2221 - val_loss: 0.2290\n",
      "Epoch 54/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2218 - val_loss: 0.2251\n",
      "Epoch 55/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2216 - val_loss: 0.2260\n",
      "Epoch 56/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2207 - val_loss: 0.2225\n",
      "Epoch 57/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2201 - val_loss: 0.2229\n",
      "Epoch 58/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2204 - val_loss: 0.2204\n",
      "Epoch 59/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2195 - val_loss: 0.2173\n",
      "Epoch 60/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2197 - val_loss: 0.2220\n",
      "Epoch 61/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2194 - val_loss: 0.2204\n",
      "Epoch 62/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2192 - val_loss: 0.2223\n",
      "Epoch 63/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2184 - val_loss: 0.2219\n",
      "Epoch 64/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2182 - val_loss: 0.2211\n",
      "Epoch 65/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2183 - val_loss: 0.2175\n",
      "Epoch 66/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2178 - val_loss: 0.2165\n",
      "Epoch 67/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2177 - val_loss: 0.2168\n",
      "Epoch 68/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2178 - val_loss: 0.2156\n",
      "Epoch 69/75\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2175 - val_loss: 0.2147\n",
      "Epoch 70/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2174 - val_loss: 0.2151\n",
      "Epoch 71/75\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2172 - val_loss: 0.2146\n",
      "Epoch 72/75\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2169 - val_loss: 0.2144\n",
      "Epoch 73/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2168 - val_loss: 0.2143\n",
      "Epoch 74/75\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2172 - val_loss: 0.2142\n",
      "Epoch 75/75\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2169 - val_loss: 0.2143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:37:47 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpmtyacjt_/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:37:51 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN\n",
      "Run name: CosineWithWarmUp_baseline_lr0.05_1723836916\n",
      "Run ID: 4ea9f1f2bbdf4355855f4ea149ff374a\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN/CosineWithWarmUp_baseline_lr0.05_1723836916\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN\n"
     ]
    }
   ],
   "source": [
    "configurations = [\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.1,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_baseline\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**14,\n",
    "        \"epochs\": 100,\n",
    "        \"initial_learning_rate\": 0.01,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_smaller_batch_lower_lr\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**16,\n",
    "        \"epochs\": 30,\n",
    "        \"initial_learning_rate\": 0.001,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_larger_batch_lowest_lr\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 75,\n",
    "        \"initial_learning_rate\": 0.05,\n",
    "        \"test_size\": 0.15,\n",
    "        \"run_name\": \"CosineWithWarmUp_medium_lr_smaller_test\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**13,\n",
    "        \"epochs\": 150,\n",
    "        \"initial_learning_rate\": 0.1,\n",
    "        \"test_size\": 0.25,\n",
    "        \"run_name\": \"CosineWithWarmUp_smallest_batch_high_epochs\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**17,\n",
    "        \"epochs\": 25,\n",
    "        \"initial_learning_rate\": 0.2,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_largest_batch_highest_lr\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**14,\n",
    "        \"epochs\": 60,\n",
    "        \"initial_learning_rate\": 0.075,\n",
    "        \"test_size\": 0.18,\n",
    "        \"run_name\": \"CosineWithWarmUp_balanced_approach\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 40,\n",
    "        \"initial_learning_rate\": 0.15,\n",
    "        \"test_size\": 0.22,\n",
    "        \"run_name\": \"CosineWithWarmUp_higher_lr_more_test\"\n",
    "    }\n",
    "]\n",
    "\n",
    "configurations2 = [\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.1,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"SELU_CosineWithWarmUp_baseline_lr0.1\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.3,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"SELU_CosineWithWarmUp_high_lr0.3\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.01,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"SELU_CosineWithWarmUp_low_lr0.01\"\n",
    "    }\n",
    "]\n",
    "\n",
    "configurations3 = [\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.05,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_baseline_lr0.05\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Example usage:\n",
    "for config in configurations3:\n",
    "    train_model(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "  1/245 [..............................] - ETA: 3:13 - loss: 4.6030WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0044s). Check your callbacks.\n",
      "245/245 [==============================] - 3s 9ms/step - loss: 0.5541 - val_loss: 1.4286\n",
      "Epoch 2/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3350 - val_loss: 0.5594\n",
      "Epoch 3/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3101 - val_loss: 0.4278\n",
      "Epoch 4/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2858 - val_loss: 0.4979\n",
      "Epoch 5/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2672 - val_loss: 0.3390\n",
      "Epoch 6/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2593 - val_loss: 0.2743\n",
      "Epoch 7/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2547 - val_loss: 0.2620\n",
      "Epoch 8/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2515 - val_loss: 0.2613\n",
      "Epoch 9/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2461 - val_loss: 0.2525\n",
      "Epoch 10/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2432 - val_loss: 0.2462\n",
      "Epoch 11/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2408 - val_loss: 0.2654\n",
      "Epoch 12/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2381 - val_loss: 0.2723\n",
      "Epoch 13/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2358 - val_loss: 0.2438\n",
      "Epoch 14/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2335 - val_loss: 0.2538\n",
      "Epoch 15/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2327 - val_loss: 0.2486\n",
      "Epoch 16/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2302 - val_loss: 0.2299\n",
      "Epoch 17/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2286 - val_loss: 0.2473\n",
      "Epoch 18/25\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2274 - val_loss: 0.2319\n",
      "Epoch 19/25\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2257 - val_loss: 0.2264\n",
      "Epoch 20/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2246 - val_loss: 0.2329\n",
      "Epoch 21/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2227 - val_loss: 0.2296\n",
      "Epoch 22/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2218 - val_loss: 0.2274\n",
      "Epoch 23/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2209 - val_loss: 0.2205\n",
      "Epoch 24/25\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2203 - val_loss: 0.2187\n",
      "Epoch 25/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2201 - val_loss: 0.2182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:40:50 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4ucvgb0_/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:40:54 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN\n",
      "Run name: CosineWithWarmUp_e25_lr0.1_1723837197\n",
      "Run ID: 32e7dc1ce3dc4669813c0be2b768fa9b\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN/CosineWithWarmUp_e25_lr0.1_1723837197\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN\n",
      "Epoch 1/25\n",
      "  1/245 [..............................] - ETA: 3:12 - loss: 2.6828WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0042s vs `on_train_batch_end` time: 0.0050s). Check your callbacks.\n",
      "245/245 [==============================] - 3s 8ms/step - loss: 0.4688 - val_loss: 0.4586\n",
      "Epoch 2/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3653 - val_loss: 0.9653\n",
      "Epoch 3/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3374 - val_loss: 0.7059\n",
      "Epoch 4/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2897 - val_loss: 0.3220\n",
      "Epoch 5/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2744 - val_loss: 0.2809\n",
      "Epoch 6/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2724 - val_loss: 0.2753\n",
      "Epoch 7/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2599 - val_loss: 0.2661\n",
      "Epoch 8/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2710 - val_loss: 0.2919\n",
      "Epoch 9/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2468 - val_loss: 0.2952\n",
      "Epoch 10/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2466 - val_loss: 0.2614\n",
      "Epoch 11/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2448 - val_loss: 0.2770\n",
      "Epoch 12/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2418 - val_loss: 0.2455\n",
      "Epoch 13/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2390 - val_loss: 0.2563\n",
      "Epoch 14/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2375 - val_loss: 0.2474\n",
      "Epoch 15/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2352 - val_loss: 0.2565\n",
      "Epoch 16/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2330 - val_loss: 0.2410\n",
      "Epoch 17/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2310 - val_loss: 0.2541\n",
      "Epoch 18/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2287 - val_loss: 0.2312\n",
      "Epoch 19/25\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2261 - val_loss: 0.2538\n",
      "Epoch 20/25\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2248 - val_loss: 0.2346\n",
      "Epoch 21/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2241 - val_loss: 0.2262\n",
      "Epoch 22/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2222 - val_loss: 0.2204\n",
      "Epoch 23/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2210 - val_loss: 0.2190\n",
      "Epoch 24/25\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2203 - val_loss: 0.2182\n",
      "Epoch 25/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2199 - val_loss: 0.2179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:41:48 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpmz3y4ity/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:41:53 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN\n",
      "Run name: CosineWithWarmUp_e25_lr0.3_1723837254\n",
      "Run ID: 5552288cfe6a4fbda1f0a49730dbdccc\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN/CosineWithWarmUp_e25_lr0.3_1723837254\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN\n",
      "Epoch 1/25\n",
      "  1/245 [..............................] - ETA: 3:38 - loss: 2.1454WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0066s). Check your callbacks.\n",
      "245/245 [==============================] - 3s 9ms/step - loss: 0.4880 - val_loss: 0.5848\n",
      "Epoch 2/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3187 - val_loss: 0.5564\n",
      "Epoch 3/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3005 - val_loss: 0.7499\n",
      "Epoch 4/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2823 - val_loss: 0.7858\n",
      "Epoch 5/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2688 - val_loss: 0.3199\n",
      "Epoch 6/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2638 - val_loss: 0.5223\n",
      "Epoch 7/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2580 - val_loss: 0.4013\n",
      "Epoch 8/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2528 - val_loss: 0.2611\n",
      "Epoch 9/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2502 - val_loss: 0.2453\n",
      "Epoch 10/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2463 - val_loss: 0.2492\n",
      "Epoch 11/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2438 - val_loss: 0.2552\n",
      "Epoch 12/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2413 - val_loss: 0.2674\n",
      "Epoch 13/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2396 - val_loss: 0.2461\n",
      "Epoch 14/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2365 - val_loss: 0.2426\n",
      "Epoch 15/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2334 - val_loss: 0.2381\n",
      "Epoch 16/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2321 - val_loss: 0.2413\n",
      "Epoch 17/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2303 - val_loss: 0.2349\n",
      "Epoch 18/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2283 - val_loss: 0.2302\n",
      "Epoch 19/25\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2274 - val_loss: 0.2357\n",
      "Epoch 20/25\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2255 - val_loss: 0.2308\n",
      "Epoch 21/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2242 - val_loss: 0.2255\n",
      "Epoch 22/25\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2229 - val_loss: 0.2234\n",
      "Epoch 23/25\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2224 - val_loss: 0.2215\n",
      "Epoch 24/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2217 - val_loss: 0.2199\n",
      "Epoch 25/25\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2216 - val_loss: 0.2197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:42:46 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp05lun0qr/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:42:50 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN\n",
      "Run name: CosineWithWarmUp_e25_lr0.05_1723837313\n",
      "Run ID: 8a009349725747b0b424dbe5f7dbef80\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN/CosineWithWarmUp_e25_lr0.05_1723837313\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN\n",
      "Epoch 1/25\n",
      "  1/245 [..............................] - ETA: 3:17 - loss: 3.8416WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_train_batch_end` time: 0.0050s). Check your callbacks.\n",
      "245/245 [==============================] - 3s 9ms/step - loss: 0.5113 - val_loss: 0.5535\n",
      "Epoch 2/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3512 - val_loss: 0.8835\n",
      "Epoch 3/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3423 - val_loss: 0.8999\n",
      "Epoch 4/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2931 - val_loss: 0.3411\n",
      "Epoch 5/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2726 - val_loss: 0.3475\n",
      "Epoch 6/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2610 - val_loss: 0.3221\n",
      "Epoch 7/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2567 - val_loss: 0.3603\n",
      "Epoch 8/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2530 - val_loss: 0.3053\n",
      "Epoch 9/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2489 - val_loss: 0.2734\n",
      "Epoch 10/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2462 - val_loss: 0.2893\n",
      "Epoch 11/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2450 - val_loss: 0.2776\n",
      "Epoch 12/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2410 - val_loss: 0.2521\n",
      "Epoch 13/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2386 - val_loss: 0.2529\n",
      "Epoch 14/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2371 - val_loss: 0.2418\n",
      "Epoch 15/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2337 - val_loss: 0.2682\n",
      "Epoch 16/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2328 - val_loss: 0.2368\n",
      "Epoch 17/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2305 - val_loss: 0.2450\n",
      "Epoch 18/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2286 - val_loss: 0.2361\n",
      "Epoch 19/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2273 - val_loss: 0.2493\n",
      "Epoch 20/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2264 - val_loss: 0.2276\n",
      "Epoch 21/25\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2244 - val_loss: 0.2254\n",
      "Epoch 22/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2235 - val_loss: 0.2236\n",
      "Epoch 23/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2227 - val_loss: 0.2225\n",
      "Epoch 24/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2221 - val_loss: 0.2203\n",
      "Epoch 25/25\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2221 - val_loss: 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:43:44 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp3imar7qc/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:43:48 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN\n",
      "Run name: CosineWithWarmUp_e25_lr0.15_1723837370\n",
      "Run ID: ff65065f04f14c419ee62fe624e38a44\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN/CosineWithWarmUp_e25_lr0.15_1723837370\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN\n",
      "Epoch 1/50\n",
      "245/245 [==============================] - 3s 8ms/step - loss: 0.5448 - val_loss: 0.5349\n",
      "Epoch 2/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3226 - val_loss: 0.7116\n",
      "Epoch 3/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3020 - val_loss: 0.5934\n",
      "Epoch 4/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3052 - val_loss: 0.5956\n",
      "Epoch 5/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2810 - val_loss: 0.2780\n",
      "Epoch 6/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2705 - val_loss: 0.2659\n",
      "Epoch 7/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2590 - val_loss: 0.2837\n",
      "Epoch 8/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2715 - val_loss: 0.3031\n",
      "Epoch 9/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2514 - val_loss: 0.3256\n",
      "Epoch 10/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2466 - val_loss: 0.2607\n",
      "Epoch 11/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2451 - val_loss: 0.2506\n",
      "Epoch 12/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2437 - val_loss: 0.2460\n",
      "Epoch 13/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2415 - val_loss: 0.2407\n",
      "Epoch 14/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2405 - val_loss: 0.2541\n",
      "Epoch 15/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2401 - val_loss: 0.2472\n",
      "Epoch 16/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2380 - val_loss: 0.2423\n",
      "Epoch 17/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2376 - val_loss: 0.2600\n",
      "Epoch 18/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2359 - val_loss: 0.2494\n",
      "Epoch 19/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2354 - val_loss: 0.2378\n",
      "Epoch 20/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2343 - val_loss: 0.2405\n",
      "Epoch 21/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2335 - val_loss: 0.2471\n",
      "Epoch 22/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2320 - val_loss: 0.2429\n",
      "Epoch 23/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2319 - val_loss: 0.2331\n",
      "Epoch 24/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2311 - val_loss: 0.2358\n",
      "Epoch 25/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2292 - val_loss: 0.2592\n",
      "Epoch 26/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2288 - val_loss: 0.2493\n",
      "Epoch 27/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2280 - val_loss: 0.2360\n",
      "Epoch 28/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2284 - val_loss: 0.2456\n",
      "Epoch 29/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2270 - val_loss: 0.2385\n",
      "Epoch 30/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2259 - val_loss: 0.2284\n",
      "Epoch 31/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2253 - val_loss: 0.2454\n",
      "Epoch 32/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2249 - val_loss: 0.2329\n",
      "Epoch 33/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2245 - val_loss: 0.2268\n",
      "Epoch 34/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2233 - val_loss: 0.2332\n",
      "Epoch 35/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2230 - val_loss: 0.2267\n",
      "Epoch 36/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2227 - val_loss: 0.2256\n",
      "Epoch 37/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2217 - val_loss: 0.2269\n",
      "Epoch 38/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2214 - val_loss: 0.2196\n",
      "Epoch 39/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2202 - val_loss: 0.2236\n",
      "Epoch 40/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2204 - val_loss: 0.2200\n",
      "Epoch 41/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2197 - val_loss: 0.2263\n",
      "Epoch 42/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2194 - val_loss: 0.2201\n",
      "Epoch 43/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2187 - val_loss: 0.2172\n",
      "Epoch 44/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2184 - val_loss: 0.2171\n",
      "Epoch 45/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2180 - val_loss: 0.2170\n",
      "Epoch 46/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2175 - val_loss: 0.2162\n",
      "Epoch 47/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2179 - val_loss: 0.2157\n",
      "Epoch 48/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2173 - val_loss: 0.2150\n",
      "Epoch 49/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2171 - val_loss: 0.2149\n",
      "Epoch 50/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2173 - val_loss: 0.2149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:45:30 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkyrlzugb/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:45:34 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN\n",
      "Run name: CosineWithWarmUp_e50_lr0.1_1723837428\n",
      "Run ID: 82294e08013546c0897b69a08010c959\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN/CosineWithWarmUp_e50_lr0.1_1723837428\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN\n",
      "Epoch 1/50\n",
      "  1/245 [..............................] - ETA: 5:40 - loss: 3.9985WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0048s vs `on_train_batch_end` time: 0.0062s). Check your callbacks.\n",
      "245/245 [==============================] - 3s 8ms/step - loss: 0.5098 - val_loss: 1.0369\n",
      "Epoch 2/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3345 - val_loss: 0.6766\n",
      "Epoch 3/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.3435 - val_loss: 0.4764\n",
      "Epoch 4/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2895 - val_loss: 0.3802\n",
      "Epoch 5/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2901 - val_loss: 0.3283\n",
      "Epoch 6/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2661 - val_loss: 0.3166\n",
      "Epoch 7/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2635 - val_loss: 0.3200\n",
      "Epoch 8/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2562 - val_loss: 0.2844\n",
      "Epoch 9/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2579 - val_loss: 0.3064\n",
      "Epoch 10/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2505 - val_loss: 0.2868\n",
      "Epoch 11/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2610 - val_loss: 0.2745\n",
      "Epoch 12/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2488 - val_loss: 0.2605\n",
      "Epoch 13/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2458 - val_loss: 0.2903\n",
      "Epoch 14/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2441 - val_loss: 0.2739\n",
      "Epoch 15/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2433 - val_loss: 0.2407\n",
      "Epoch 16/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 14.2616 - val_loss: 31.2884\n",
      "Epoch 17/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.4694 - val_loss: 0.5376\n",
      "Epoch 18/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3980 - val_loss: 0.5712\n",
      "Epoch 19/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3402 - val_loss: 0.3952\n",
      "Epoch 20/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3202 - val_loss: 0.3348\n",
      "Epoch 21/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.3013 - val_loss: 0.3552\n",
      "Epoch 22/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2902 - val_loss: 0.3162\n",
      "Epoch 23/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2870 - val_loss: 0.2800\n",
      "Epoch 24/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2975 - val_loss: 0.3174\n",
      "Epoch 25/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2919 - val_loss: 0.3886\n",
      "Epoch 26/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2913 - val_loss: 0.2735\n",
      "Epoch 27/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2942 - val_loss: 0.2834\n",
      "Epoch 28/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2755 - val_loss: 0.2747\n",
      "Epoch 29/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2765 - val_loss: 0.2604\n",
      "Epoch 30/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2659 - val_loss: 0.2628\n",
      "Epoch 31/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2695 - val_loss: 0.2855\n",
      "Epoch 32/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2548 - val_loss: 0.2615\n",
      "Epoch 33/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2526 - val_loss: 0.2831\n",
      "Epoch 34/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2499 - val_loss: 0.2460\n",
      "Epoch 35/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2429 - val_loss: 0.2435\n",
      "Epoch 36/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2430 - val_loss: 0.2543\n",
      "Epoch 37/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2405 - val_loss: 0.2561\n",
      "Epoch 38/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2393 - val_loss: 0.2429\n",
      "Epoch 39/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2370 - val_loss: 0.2420\n",
      "Epoch 40/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2346 - val_loss: 0.2434\n",
      "Epoch 41/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2338 - val_loss: 0.2314\n",
      "Epoch 42/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2330 - val_loss: 0.2318\n",
      "Epoch 43/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2320 - val_loss: 0.2301\n",
      "Epoch 44/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2313 - val_loss: 0.2299\n",
      "Epoch 45/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2308 - val_loss: 0.2293\n",
      "Epoch 46/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2304 - val_loss: 0.2286\n",
      "Epoch 47/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2297 - val_loss: 0.2280\n",
      "Epoch 48/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2298 - val_loss: 0.2279\n",
      "Epoch 49/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2294 - val_loss: 0.2277\n",
      "Epoch 50/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2294 - val_loss: 0.2277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:47:20 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpj72fil8e/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:47:24 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN\n",
      "Run name: CosineWithWarmUp_e50_lr0.3_1723837534\n",
      "Run ID: c1793ba4ddcf4cdbae6c98d478e016e4\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN/CosineWithWarmUp_e50_lr0.3_1723837534\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN\n",
      "Epoch 1/50\n",
      "  1/245 [..............................] - ETA: 3:37 - loss: 3.7975WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0032s vs `on_train_batch_end` time: 0.0052s). Check your callbacks.\n",
      "245/245 [==============================] - 3s 9ms/step - loss: 0.5797 - val_loss: 0.4784\n",
      "Epoch 2/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3426 - val_loss: 0.4465\n",
      "Epoch 3/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3237 - val_loss: 0.9041\n",
      "Epoch 4/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2886 - val_loss: 0.3667\n",
      "Epoch 5/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2886 - val_loss: 0.3723\n",
      "Epoch 6/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2780 - val_loss: 0.5038\n",
      "Epoch 7/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2695 - val_loss: 0.3008\n",
      "Epoch 8/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2636 - val_loss: 0.3484\n",
      "Epoch 9/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2604 - val_loss: 0.2845\n",
      "Epoch 10/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2565 - val_loss: 0.3243\n",
      "Epoch 11/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2553 - val_loss: 0.2638\n",
      "Epoch 12/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2532 - val_loss: 0.3039\n",
      "Epoch 13/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2502 - val_loss: 0.2776\n",
      "Epoch 14/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2484 - val_loss: 0.2505\n",
      "Epoch 15/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2472 - val_loss: 0.2522\n",
      "Epoch 16/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2446 - val_loss: 0.2621\n",
      "Epoch 17/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2443 - val_loss: 0.2696\n",
      "Epoch 18/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2417 - val_loss: 0.2506\n",
      "Epoch 19/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2407 - val_loss: 0.2601\n",
      "Epoch 20/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2400 - val_loss: 0.2507\n",
      "Epoch 21/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2381 - val_loss: 0.2564\n",
      "Epoch 22/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2381 - val_loss: 0.2459\n",
      "Epoch 23/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2370 - val_loss: 0.2521\n",
      "Epoch 24/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2358 - val_loss: 0.2385\n",
      "Epoch 25/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2346 - val_loss: 0.2446\n",
      "Epoch 26/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2331 - val_loss: 0.2528\n",
      "Epoch 27/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2333 - val_loss: 0.2370\n",
      "Epoch 28/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2319 - val_loss: 0.2395\n",
      "Epoch 29/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2306 - val_loss: 0.2425\n",
      "Epoch 30/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2301 - val_loss: 0.2450\n",
      "Epoch 31/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2294 - val_loss: 0.2500\n",
      "Epoch 32/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2283 - val_loss: 0.2398\n",
      "Epoch 33/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2276 - val_loss: 0.2722\n",
      "Epoch 34/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2271 - val_loss: 0.2263\n",
      "Epoch 35/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2255 - val_loss: 0.2235\n",
      "Epoch 36/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2245 - val_loss: 0.2329\n",
      "Epoch 37/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2249 - val_loss: 0.2305\n",
      "Epoch 38/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2239 - val_loss: 0.2249\n",
      "Epoch 39/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2232 - val_loss: 0.2323\n",
      "Epoch 40/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2225 - val_loss: 0.2310\n",
      "Epoch 41/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2220 - val_loss: 0.2276\n",
      "Epoch 42/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2216 - val_loss: 0.2206\n",
      "Epoch 43/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2212 - val_loss: 0.2212\n",
      "Epoch 44/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2205 - val_loss: 0.2215\n",
      "Epoch 45/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2202 - val_loss: 0.2191\n",
      "Epoch 46/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2200 - val_loss: 0.2182\n",
      "Epoch 47/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2198 - val_loss: 0.2179\n",
      "Epoch 48/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2195 - val_loss: 0.2173\n",
      "Epoch 49/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2194 - val_loss: 0.2172\n",
      "Epoch 50/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2193 - val_loss: 0.2172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:49:06 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpcrmeq8qp/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:49:10 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN\n",
      "Run name: CosineWithWarmUp_e50_lr0.05_1723837644\n",
      "Run ID: e3b64d48c9374b02b6e9f4ae6f18292d\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN/CosineWithWarmUp_e50_lr0.05_1723837644\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN\n",
      "Epoch 1/50\n",
      "245/245 [==============================] - 3s 8ms/step - loss: 0.5366 - val_loss: 0.5733\n",
      "Epoch 2/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3425 - val_loss: 0.5971\n",
      "Epoch 3/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3162 - val_loss: 0.4460\n",
      "Epoch 4/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.3188 - val_loss: 0.5761\n",
      "Epoch 5/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2967 - val_loss: 0.3987\n",
      "Epoch 6/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2722 - val_loss: 0.2972\n",
      "Epoch 7/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2661 - val_loss: 0.3318\n",
      "Epoch 8/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2564 - val_loss: 0.2950\n",
      "Epoch 9/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2550 - val_loss: 0.2981\n",
      "Epoch 10/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2491 - val_loss: 0.3559\n",
      "Epoch 11/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2472 - val_loss: 0.2785\n",
      "Epoch 12/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2451 - val_loss: 0.2743\n",
      "Epoch 13/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2446 - val_loss: 0.2579\n",
      "Epoch 14/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2421 - val_loss: 0.2414\n",
      "Epoch 15/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2409 - val_loss: 0.2451\n",
      "Epoch 16/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2401 - val_loss: 0.2509\n",
      "Epoch 17/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2397 - val_loss: 0.2424\n",
      "Epoch 18/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2376 - val_loss: 0.2636\n",
      "Epoch 19/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2374 - val_loss: 0.2455\n",
      "Epoch 20/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2360 - val_loss: 0.2678\n",
      "Epoch 21/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2351 - val_loss: 0.2549\n",
      "Epoch 22/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2341 - val_loss: 0.2472\n",
      "Epoch 23/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2334 - val_loss: 0.2564\n",
      "Epoch 24/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2325 - val_loss: 0.2389\n",
      "Epoch 25/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2313 - val_loss: 0.2321\n",
      "Epoch 26/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2313 - val_loss: 0.2530\n",
      "Epoch 27/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2299 - val_loss: 0.2323\n",
      "Epoch 28/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2292 - val_loss: 0.2329\n",
      "Epoch 29/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2278 - val_loss: 0.2392\n",
      "Epoch 30/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2270 - val_loss: 0.2258\n",
      "Epoch 31/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2267 - val_loss: 0.2289\n",
      "Epoch 32/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2248 - val_loss: 0.2502\n",
      "Epoch 33/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2250 - val_loss: 0.2291\n",
      "Epoch 34/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2242 - val_loss: 0.2283\n",
      "Epoch 35/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2231 - val_loss: 0.2346\n",
      "Epoch 36/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2226 - val_loss: 0.2237\n",
      "Epoch 37/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2220 - val_loss: 0.2223\n",
      "Epoch 38/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2214 - val_loss: 0.2193\n",
      "Epoch 39/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2207 - val_loss: 0.2233\n",
      "Epoch 40/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2199 - val_loss: 0.2199\n",
      "Epoch 41/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2197 - val_loss: 0.2179\n",
      "Epoch 42/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2189 - val_loss: 0.2201\n",
      "Epoch 43/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2182 - val_loss: 0.2190\n",
      "Epoch 44/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2181 - val_loss: 0.2188\n",
      "Epoch 45/50\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.2177 - val_loss: 0.2158\n",
      "Epoch 46/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2176 - val_loss: 0.2154\n",
      "Epoch 47/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2174 - val_loss: 0.2145\n",
      "Epoch 48/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2165 - val_loss: 0.2145\n",
      "Epoch 49/50\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.2168 - val_loss: 0.2143\n",
      "Epoch 50/50\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.2170 - val_loss: 0.2143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:50:52 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpi0i20ln6/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/16 19:50:57 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed and logged with MLflow and TensorBoard.\n",
      "Experiment name: Inverse Kinematics NN\n",
      "Run name: CosineWithWarmUp_e50_lr0.15_1723837750\n",
      "Run ID: 62ef56344f1a440485a56d457022dcbe\n",
      "TensorBoard logs saved to: logs/Inverse Kinematics NN/CosineWithWarmUp_e50_lr0.15_1723837750\n",
      "To view in TensorBoard, run:\n",
      "tensorboard --logdir logs/Inverse Kinematics NN\n"
     ]
    }
   ],
   "source": [
    "configurations = [\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 25,\n",
    "        \"initial_learning_rate\": 0.1,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_e25_lr0.1\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 25,\n",
    "        \"initial_learning_rate\": 0.3,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_e25_lr0.3\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 25,\n",
    "        \"initial_learning_rate\": 0.05,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_e25_lr0.05\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 25,\n",
    "        \"initial_learning_rate\": 0.15,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_e25_lr0.15\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.1,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_e50_lr0.1\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.3,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_e50_lr0.3\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.05,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_e50_lr0.05\"\n",
    "    },\n",
    "    {\n",
    "        \"batch_size\": 2**15,\n",
    "        \"epochs\": 50,\n",
    "        \"initial_learning_rate\": 0.15,\n",
    "        \"test_size\": 0.2,\n",
    "        \"run_name\": \"CosineWithWarmUp_e50_lr0.15\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Example usage:\n",
    "for config in configurations:\n",
    "    train_model(**config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
