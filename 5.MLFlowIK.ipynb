{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import os\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "L1, L2, L3 = 1.0, 1.5, 0.5  # link lengths\n",
    "\n",
    "@tf.function\n",
    "def forward_kinematics_tf(theta):\n",
    "    theta1, theta2, theta3 = tf.unstack(theta, axis=1)\n",
    "    \n",
    "    x = L1 * tf.cos(theta1) * tf.sin(theta2) + L2 * tf.cos(theta1) * tf.sin(theta2 + theta3)\n",
    "    y = L1 * tf.sin(theta1) * tf.sin(theta2) + L2 * tf.sin(theta1) * tf.sin(theta2 + theta3)\n",
    "    z = L1 * tf.cos(theta2) + L2 * tf.cos(theta2 + theta3) + L3\n",
    "    \n",
    "    return tf.stack([x, y, z], axis=1)\n",
    "\n",
    "def evaluate_model(model, test_inputs, test_outputs, input_mean, input_std, batch_size=2**16):\n",
    "    test_inputs = tf.convert_to_tensor(test_inputs, dtype=tf.float32)\n",
    "    input_mean = tf.convert_to_tensor(input_mean, dtype=tf.float32)\n",
    "    input_std = tf.convert_to_tensor(input_std, dtype=tf.float32)\n",
    "    predicted_angles_normalized = model.predict(test_inputs, batch_size=batch_size)\n",
    "    predicted_angles = predicted_angles_normalized * (np.pi/2)\n",
    "    true_xyz = test_inputs * input_std + input_mean\n",
    "    predicted_xyz = forward_kinematics_tf(predicted_angles)\n",
    "    errors = tf.norm(true_xyz - predicted_xyz, axis=1)\n",
    "    return errors.numpy(), true_xyz.numpy(), predicted_xyz.numpy()\n",
    "\n",
    "def custom_loss(fk_weight=1, delta=0.1):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Huber loss for joint angles\n",
    "        angle_loss = huber_loss(y_true, y_pred, delta)\n",
    "        \n",
    "        # Forward kinematics loss (using Huber loss)\n",
    "        fk_true = forward_kinematics_tf(y_true)\n",
    "        fk_pred = forward_kinematics_tf(y_pred)\n",
    "        fk_loss = huber_loss(fk_true, fk_pred, delta)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = tf.reduce_mean(angle_loss) + fk_weight * tf.reduce_mean(fk_loss)\n",
    "        return total_loss\n",
    "    return loss_fn\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) <= delta\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = delta * (tf.abs(error) - delta / 2)\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "def huber_exp_loss(y_true, y_pred, delta=0.2, alpha=1):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) <= delta\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    exp_loss = alpha * (tf.exp(tf.abs(error) - delta) - 1) + delta * tf.abs(error) - delta**2 / 2\n",
    "    return tf.where(is_small_error, squared_loss, exp_loss)\n",
    "\n",
    "def log_cosh_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.math.log(tf.math.cosh(y_pred - y_true)))\n",
    "\n",
    "def create_model(config):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, activation='relu', input_shape=(3,)),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(3, activation='tanh')\n",
    "    ])\n",
    "    \n",
    "    loss_type = config['loss_type']\n",
    "    if loss_type == 'standard':\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "    elif loss_type == 'custom':\n",
    "        model.compile(optimizer='adam', loss=custom_loss())\n",
    "    elif loss_type == 'modified_custom':\n",
    "        model.compile(optimizer='adam', loss=custom_loss(fk_weight=config.get('fk_weight', 10)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_and_preprocess_data(filename='robot_arm_dataset_10M.npz'):\n",
    "    data = np.load(f'./Data/{filename}')\n",
    "    inputs, outputs = data['inputs'], data['outputs']\n",
    "    \n",
    "    input_mean = np.mean(inputs, axis=0)\n",
    "    input_std = np.std(inputs, axis=0)\n",
    "    inputs_normalized = (inputs - input_mean) / input_std\n",
    "\n",
    "    outputs_normalized = outputs / (np.pi/2)\n",
    "\n",
    "    split_index = int(0.9 * len(inputs))\n",
    "    train_inputs, test_inputs = inputs_normalized[:split_index], inputs_normalized[split_index:]\n",
    "    train_outputs, test_outputs = outputs_normalized[:split_index], outputs_normalized[split_index:]\n",
    "\n",
    "    return (train_inputs, train_outputs), (test_inputs, test_outputs), input_mean, input_std\n",
    "\n",
    "class VerboseLoggingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = io.StringIO()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        output = f\"Epoch {epoch+1}/{self.params['epochs']} - \"\n",
    "        output += \" - \".join(f\"{k}: {v:.4f}\" for k, v in logs.items())\n",
    "        print(output)\n",
    "        self.output.write(output + \"\\n\")\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.output.getvalue()\n",
    "\n",
    "class LearningRateLogger(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'value'):\n",
    "            lr = lr.value()\n",
    "        mlflow.log_metric(\"learning_rate\", lr, step=epoch)\n",
    "        \n",
    "class CosineDecayWithWarmupCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_learning_rate, warmup_steps, total_steps):\n",
    "        super(CosineDecayWithWarmupCallback, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            lr = self.initial_learning_rate * (self.current_step / self.warmup_steps)\n",
    "        else:\n",
    "            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            lr = 0.5 * self.initial_learning_rate * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        self.current_step += 1\n",
    "\n",
    "def plot_error_distribution(errors, title, save_path=None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(errors, bins=400, alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Error (Euclidean distance)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlim([0, 0.5])\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_true_vs_predicted(true_xyz, predicted_xyz, title, save_path=None):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    for i, coord in enumerate(['X', 'Y', 'Z']):\n",
    "        ax = axes[i]\n",
    "        ax.scatter(true_xyz[:, i], predicted_xyz[:, i], alpha=0.1)\n",
    "        ax.plot([true_xyz[:, i].min(), true_xyz[:, i].max()], [true_xyz[:, i].min(), true_xyz[:, i].max()], 'r--')\n",
    "        ax.set_xlabel(f'True {coord}')\n",
    "        ax.set_ylabel(f'Predicted {coord}')\n",
    "        ax.set_title(f'{coord} Coordinate: True vs Predicted')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "def train_and_evaluate_model(train_data, valid_data, test_data, input_mean, input_std, config):\n",
    "    train_inputs, train_outputs = train_data\n",
    "    valid_inputs, valid_outputs = valid_data\n",
    "    test_inputs, test_outputs = test_data\n",
    "    \n",
    "    experiment_name = config['experiment_name']\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=config['model_name']):\n",
    "        # Calculate total steps\n",
    "        steps_per_epoch = len(train_inputs) // config['batch_size']\n",
    "        total_steps = steps_per_epoch * epochs\n",
    "        warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(config)\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = create_model(config)\n",
    "        \n",
    "        # Log model summary\n",
    "        model_summary = io.StringIO()\n",
    "        model.summary(print_fn=lambda x: model_summary.write(x + '\\n'))\n",
    "        mlflow.log_text(model_summary.getvalue(), \"model_summary.txt\")\n",
    "        \n",
    "        # Set up callbacks\n",
    "        early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        \n",
    "        lr_scheduler = CosineDecayWithWarmupCallback(\n",
    "            config['initial_learning_rate'],\n",
    "            warmup_steps,\n",
    "            total_steps\n",
    "        )\n",
    "        \n",
    "        verbose_logging = VerboseLoggingCallback()\n",
    "        lr_logger = LearningRateLogger()\n",
    "        \n",
    "        callbacks = [early_stopping, lr_scheduler, verbose_logging, lr_logger]\n",
    "        \n",
    "        # Log callback names\n",
    "        callback_names = [callback.__class__.__name__ for callback in callbacks]\n",
    "        mlflow.log_param(\"callbacks\", \", \".join(callback_names))\n",
    "        \n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            train_inputs, train_outputs,\n",
    "            epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            validation_data=(valid_inputs, valid_outputs),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Log training metrics\n",
    "        for epoch, (loss, val_loss) in enumerate(zip(history.history['loss'], history.history['val_loss'])):\n",
    "            mlflow.log_metric(\"train_loss\", loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        \n",
    "        mlflow.log_metric(\"training_time\", training_time)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.tensorflow.log_model(model, \"model\")\n",
    "        \n",
    "        # Log training output\n",
    "        mlflow.log_text(verbose_logging.get_output(), \"training_output.txt\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        errors, true_xyz, predicted_xyz = evaluate_model(model, test_inputs, test_outputs, input_mean, input_std)\n",
    "        \n",
    "        # Log evaluation metrics\n",
    "        mlflow.log_metric(\"mean_error\", np.mean(errors))\n",
    "        mlflow.log_metric(\"median_error\", np.median(errors))\n",
    "        mlflow.log_metric(\"90th_percentile_error\", np.percentile(errors, 90))\n",
    "        mlflow.log_metric(\"max_error\", np.max(errors))\n",
    "        \n",
    "        # Generate and log the true vs predicted plot\n",
    "        true_vs_pred_plot_path = f\"{config['model_name']}_true_vs_predicted.png\"\n",
    "        plot_true_vs_predicted(true_xyz, predicted_xyz, f\"{config['model_name']} Model: True vs Predicted\", save_path=true_vs_pred_plot_path)\n",
    "        mlflow.log_artifact(true_vs_pred_plot_path)\n",
    "        os.remove(true_vs_pred_plot_path)  # Clean up the temporary file\n",
    "        \n",
    "        # Generate and log the error distribution plot\n",
    "        error_dist_plot_path = f\"{config['model_name']}_error_distribution.png\"\n",
    "        plot_error_distribution(errors, f\"{config['model_name']} Model: Error Distribution\", save_path=error_dist_plot_path)\n",
    "        mlflow.log_artifact(error_dist_plot_path)\n",
    "        os.remove(error_dist_plot_path)  # Clean up the temporary file\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\n{config['model_name']} Model:\")\n",
    "        print(f\"Mean Error: {np.mean(errors):.4f}\")\n",
    "        print(f\"Median Error: {np.median(errors):.4f}\")\n",
    "        print(f\"90th Percentile Error: {np.percentile(errors, 90):.4f}\")\n",
    "        print(f\"Max Error: {np.max(errors):.4f}\")\n",
    "        print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'errors': errors,\n",
    "            'true_xyz': true_xyz,\n",
    "            'predicted_xyz': predicted_xyz,\n",
    "            'training_time': training_time\n",
    "        }\n",
    "\n",
    "def run_single_experiment(config):\n",
    "    (train_inputs, train_outputs), (test_inputs, test_outputs), input_mean, input_std = load_and_preprocess_data()\n",
    "    \n",
    "    # Split test data into validation and test sets\n",
    "    valid_inputs, test_inputs, valid_outputs, test_outputs = train_test_split(\n",
    "        test_inputs, test_outputs, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_and_evaluate_model(\n",
    "        (train_inputs, train_outputs),\n",
    "        (valid_inputs, valid_outputs),\n",
    "        (test_inputs, test_outputs),\n",
    "        input_mean, input_std,\n",
    "        config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 317, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 410, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1341, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1334, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/utils/file_utils.py\", line 309, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.2508Epoch 1/100 - loss: 0.2485 - val_loss: 0.1130\n",
      "138/138 [==============================] - 3s 11ms/step - loss: 0.2485 - val_loss: 0.1130\n",
      "Epoch 2/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0782Epoch 2/100 - loss: 0.0776 - val_loss: 0.0519\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0776 - val_loss: 0.0519\n",
      "Epoch 3/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0429Epoch 3/100 - loss: 0.0427 - val_loss: 0.0369\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0427 - val_loss: 0.0369\n",
      "Epoch 4/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0345Epoch 4/100 - loss: 0.0345 - val_loss: 0.0321\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0345 - val_loss: 0.0321\n",
      "Epoch 5/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0314Epoch 5/100 - loss: 0.0314 - val_loss: 0.0316\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0314 - val_loss: 0.0316\n",
      "Epoch 6/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0297Epoch 6/100 - loss: 0.0297 - val_loss: 0.0296\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0297 - val_loss: 0.0296\n",
      "Epoch 7/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0284Epoch 7/100 - loss: 0.0284 - val_loss: 0.0271\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0284 - val_loss: 0.0271\n",
      "Epoch 8/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0255Epoch 8/100 - loss: 0.0255 - val_loss: 0.0226\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0255 - val_loss: 0.0226\n",
      "Epoch 9/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0201Epoch 9/100 - loss: 0.0201 - val_loss: 0.0184\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0201 - val_loss: 0.0184\n",
      "Epoch 10/100\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0184Epoch 10/100 - loss: 0.0184 - val_loss: 0.0171\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0184 - val_loss: 0.0171\n",
      "Epoch 11/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0173Epoch 11/100 - loss: 0.0173 - val_loss: 0.0165\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0173 - val_loss: 0.0165\n",
      "Epoch 12/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0166Epoch 12/100 - loss: 0.0165 - val_loss: 0.0160\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0165 - val_loss: 0.0160\n",
      "Epoch 13/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0163Epoch 13/100 - loss: 0.0162 - val_loss: 0.0157\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0162 - val_loss: 0.0157\n",
      "Epoch 14/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0159Epoch 14/100 - loss: 0.0159 - val_loss: 0.0179\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0159 - val_loss: 0.0179\n",
      "Epoch 15/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0157Epoch 15/100 - loss: 0.0157 - val_loss: 0.0154\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0157 - val_loss: 0.0154\n",
      "Epoch 16/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0155Epoch 16/100 - loss: 0.0155 - val_loss: 0.0154\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 17/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0154Epoch 17/100 - loss: 0.0154 - val_loss: 0.0151\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0154 - val_loss: 0.0151\n",
      "Epoch 18/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0154Epoch 18/100 - loss: 0.0154 - val_loss: 0.0152\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0154 - val_loss: 0.0152\n",
      "Epoch 19/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0152Epoch 19/100 - loss: 0.0152 - val_loss: 0.0150\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0152 - val_loss: 0.0150\n",
      "Epoch 20/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0152Epoch 20/100 - loss: 0.0152 - val_loss: 0.0154\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0152 - val_loss: 0.0154\n",
      "Epoch 21/100\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0151Epoch 21/100 - loss: 0.0151 - val_loss: 0.0156\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0151 - val_loss: 0.0156\n",
      "Epoch 22/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0151Epoch 22/100 - loss: 0.0151 - val_loss: 0.0150\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0151 - val_loss: 0.0150\n",
      "Epoch 23/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 23/100 - loss: 0.0150 - val_loss: 0.0151\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0150 - val_loss: 0.0151\n",
      "Epoch 24/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0150Epoch 24/100 - loss: 0.0150 - val_loss: 0.0149\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0150 - val_loss: 0.0149\n",
      "Epoch 25/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0149Epoch 25/100 - loss: 0.0149 - val_loss: 0.0147\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0149 - val_loss: 0.0147\n",
      "Epoch 26/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0149Epoch 26/100 - loss: 0.0149 - val_loss: 0.0148\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0149 - val_loss: 0.0148\n",
      "Epoch 27/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0148Epoch 27/100 - loss: 0.0148 - val_loss: 0.0153\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0148 - val_loss: 0.0153\n",
      "Epoch 28/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0148Epoch 28/100 - loss: 0.0148 - val_loss: 0.0147\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0148 - val_loss: 0.0147\n",
      "Epoch 29/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0149Epoch 29/100 - loss: 0.0149 - val_loss: 0.0147\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0149 - val_loss: 0.0147\n",
      "Epoch 30/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0147Epoch 30/100 - loss: 0.0147 - val_loss: 0.0148\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0147 - val_loss: 0.0148\n",
      "Epoch 31/100\n",
      "132/138 [===========================>..] - ETA: 0s - loss: 0.0148Epoch 31/100 - loss: 0.0148 - val_loss: 0.0147\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0148 - val_loss: 0.0147\n",
      "Epoch 32/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0147Epoch 32/100 - loss: 0.0147 - val_loss: 0.0146\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0147 - val_loss: 0.0146\n",
      "Epoch 33/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0147Epoch 33/100 - loss: 0.0147 - val_loss: 0.0147\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0147 - val_loss: 0.0147\n",
      "Epoch 34/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0147Epoch 34/100 - loss: 0.0148 - val_loss: 0.0147\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0148 - val_loss: 0.0147\n",
      "Epoch 35/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0147Epoch 35/100 - loss: 0.0147 - val_loss: 0.0145\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0147 - val_loss: 0.0145\n",
      "Epoch 36/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0147Epoch 36/100 - loss: 0.0147 - val_loss: 0.0146\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0147 - val_loss: 0.0146\n",
      "Epoch 37/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0147Epoch 37/100 - loss: 0.0147 - val_loss: 0.0145\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0147 - val_loss: 0.0145\n",
      "Epoch 38/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0146Epoch 38/100 - loss: 0.0146 - val_loss: 0.0148\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0146 - val_loss: 0.0148\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0147Epoch 39/100 - loss: 0.0146 - val_loss: 0.0147\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 40/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0146Epoch 40/100 - loss: 0.0146 - val_loss: 0.0145\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0146 - val_loss: 0.0145\n",
      "Epoch 41/100\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0145Epoch 41/100 - loss: 0.0145 - val_loss: 0.0145\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0145 - val_loss: 0.0145\n",
      "Epoch 42/100\n",
      "132/138 [===========================>..] - ETA: 0s - loss: 0.0146Epoch 42/100 - loss: 0.0146 - val_loss: 0.0146\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0146 - val_loss: 0.0146\n",
      "Epoch 43/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0145Epoch 43/100 - loss: 0.0145 - val_loss: 0.0146\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0145 - val_loss: 0.0146\n",
      "Epoch 44/100\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0146Epoch 44/100 - loss: 0.0146 - val_loss: 0.0146\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0146 - val_loss: 0.0146\n",
      "Epoch 45/100\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0146Epoch 45/100 - loss: 0.0146 - val_loss: 0.0148\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0146 - val_loss: 0.0148\n",
      "Epoch 46/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0145Epoch 46/100 - loss: 0.0145 - val_loss: 0.0145\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0145 - val_loss: 0.0145\n",
      "Epoch 47/100\n",
      "132/138 [===========================>..] - ETA: 0s - loss: 0.0145Epoch 47/100 - loss: 0.0145 - val_loss: 0.0144\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0145 - val_loss: 0.0144\n",
      "Epoch 48/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0145Epoch 48/100 - loss: 0.0145 - val_loss: 0.0145\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0145 - val_loss: 0.0145\n",
      "Epoch 49/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0145Epoch 49/100 - loss: 0.0145 - val_loss: 0.0145\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0145 - val_loss: 0.0145\n",
      "Epoch 50/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0145Epoch 50/100 - loss: 0.0145 - val_loss: 0.0145\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0145 - val_loss: 0.0145\n",
      "Epoch 51/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0144Epoch 51/100 - loss: 0.0144 - val_loss: 0.0145\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0144 - val_loss: 0.0145\n",
      "Epoch 52/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0144Epoch 52/100 - loss: 0.0144 - val_loss: 0.0144\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0144 - val_loss: 0.0144\n",
      "Epoch 53/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0144Epoch 53/100 - loss: 0.0144 - val_loss: 0.0146\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0144 - val_loss: 0.0146\n",
      "Epoch 54/100\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0144Epoch 54/100 - loss: 0.0144 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 55/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0144Epoch 55/100 - loss: 0.0144 - val_loss: 0.0144\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0144 - val_loss: 0.0144\n",
      "Epoch 56/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0144Epoch 56/100 - loss: 0.0144 - val_loss: 0.0144\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0144 - val_loss: 0.0144\n",
      "Epoch 57/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0144Epoch 57/100 - loss: 0.0144 - val_loss: 0.0144\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0144 - val_loss: 0.0144\n",
      "Epoch 58/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0144Epoch 58/100 - loss: 0.0144 - val_loss: 0.0145\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0144 - val_loss: 0.0145\n",
      "Epoch 59/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0144Epoch 59/100 - loss: 0.0144 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 60/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0143Epoch 60/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 61/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0144Epoch 61/100 - loss: 0.0144 - val_loss: 0.0144\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0144 - val_loss: 0.0144\n",
      "Epoch 62/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0144Epoch 62/100 - loss: 0.0144 - val_loss: 0.0144\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0144 - val_loss: 0.0144\n",
      "Epoch 63/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0143Epoch 63/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 64/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0143Epoch 64/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 65/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0143Epoch 65/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 66/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0143Epoch 66/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 67/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0143Epoch 67/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 68/100\n",
      "132/138 [===========================>..] - ETA: 0s - loss: 0.0143Epoch 68/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 69/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0143Epoch 69/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 70/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0143Epoch 70/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 71/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0143Epoch 71/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 72/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0143Epoch 72/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 73/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0143Epoch 73/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 74/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0143Epoch 74/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0143 - val_loss: 0.0143\n",
      "Epoch 75/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0143Epoch 75/100 - loss: 0.0143 - val_loss: 0.0143\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0143 - val_loss: 0.0143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/20 01:41:04 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpiz5_2_4d/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpiz5_2_4d/model/data/model/assets\n",
      "2024/08/20 01:41:08 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpiz5_2_4d/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "/usr/local/lib/python3.8/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2024/08/20 01:41:08 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n",
      "\n",
      "Huber Loss (both), fk=5e100,d01 Model:\n",
      "Mean Error: 0.0330\n",
      "Median Error: 0.0321\n",
      "90th Percentile Error: 0.0574\n",
      "Max Error: 4.0006\n",
      "Training Time: 100.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 317, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 410, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1341, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/store/tracking/file_store.py\", line 1334, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/mlflow/utils/file_utils.py\", line 309, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/tf/workdir/mlruns/mlruns/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.4323Epoch 1/100 - loss: 0.4281 - val_loss: 0.1801\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 0.4281 - val_loss: 0.1801\n",
      "Epoch 2/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.1239Epoch 2/100 - loss: 0.1230 - val_loss: 0.0818\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.1230 - val_loss: 0.0818\n",
      "Epoch 3/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0650Epoch 3/100 - loss: 0.0647 - val_loss: 0.0535\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0647 - val_loss: 0.0535\n",
      "Epoch 4/100\n",
      "132/138 [===========================>..] - ETA: 0s - loss: 0.0489Epoch 4/100 - loss: 0.0488 - val_loss: 0.0447\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0488 - val_loss: 0.0447\n",
      "Epoch 5/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0431Epoch 5/100 - loss: 0.0431 - val_loss: 0.0417\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0431 - val_loss: 0.0417\n",
      "Epoch 6/100\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0400Epoch 6/100 - loss: 0.0400 - val_loss: 0.0385\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0400 - val_loss: 0.0385\n",
      "Epoch 7/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0367Epoch 7/100 - loss: 0.0367 - val_loss: 0.0329\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0367 - val_loss: 0.0329\n",
      "Epoch 8/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0299Epoch 8/100 - loss: 0.0299 - val_loss: 0.0259\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0299 - val_loss: 0.0259\n",
      "Epoch 9/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0238Epoch 9/100 - loss: 0.0238 - val_loss: 0.0225\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0238 - val_loss: 0.0225\n",
      "Epoch 10/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0214Epoch 10/100 - loss: 0.0214 - val_loss: 0.0228\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0214 - val_loss: 0.0228\n",
      "Epoch 11/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0194Epoch 11/100 - loss: 0.0194 - val_loss: 0.0188\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0194 - val_loss: 0.0188\n",
      "Epoch 12/100\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0186Epoch 12/100 - loss: 0.0186 - val_loss: 0.0216\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0186 - val_loss: 0.0216\n",
      "Epoch 13/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0182Epoch 13/100 - loss: 0.0182 - val_loss: 0.0172\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0182 - val_loss: 0.0172\n",
      "Epoch 14/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0176Epoch 14/100 - loss: 0.0176 - val_loss: 0.0184\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0176 - val_loss: 0.0184\n",
      "Epoch 15/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0174Epoch 15/100 - loss: 0.0174 - val_loss: 0.0173\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0174 - val_loss: 0.0173\n",
      "Epoch 16/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0169Epoch 16/100 - loss: 0.0169 - val_loss: 0.0168\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 17/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0169Epoch 17/100 - loss: 0.0169 - val_loss: 0.0169\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 18/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0166Epoch 18/100 - loss: 0.0166 - val_loss: 0.0164\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0166 - val_loss: 0.0164\n",
      "Epoch 19/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0163Epoch 19/100 - loss: 0.0163 - val_loss: 0.0159\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0163 - val_loss: 0.0159\n",
      "Epoch 20/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0163Epoch 20/100 - loss: 0.0163 - val_loss: 0.0176\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0163 - val_loss: 0.0176\n",
      "Epoch 21/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0162Epoch 21/100 - loss: 0.0162 - val_loss: 0.0158\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0162 - val_loss: 0.0158\n",
      "Epoch 22/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0160Epoch 22/100 - loss: 0.0161 - val_loss: 0.0174\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0161 - val_loss: 0.0174\n",
      "Epoch 23/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0159Epoch 23/100 - loss: 0.0159 - val_loss: 0.0163\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0159 - val_loss: 0.0163\n",
      "Epoch 24/100\n",
      "132/138 [===========================>..] - ETA: 0s - loss: 0.0160Epoch 24/100 - loss: 0.0160 - val_loss: 0.0163\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0160 - val_loss: 0.0163\n",
      "Epoch 25/100\n",
      "136/138 [============================>.] - ETA: 0s - loss: 0.0158Epoch 25/100 - loss: 0.0158 - val_loss: 0.0158\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0158 - val_loss: 0.0158\n",
      "Epoch 26/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0158Epoch 26/100 - loss: 0.0158 - val_loss: 0.0155\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0158 - val_loss: 0.0155\n",
      "Epoch 27/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0157Epoch 27/100 - loss: 0.0157 - val_loss: 0.0157\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 28/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0157Epoch 28/100 - loss: 0.0157 - val_loss: 0.0159\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0157 - val_loss: 0.0159\n",
      "Epoch 29/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0156Epoch 29/100 - loss: 0.0156 - val_loss: 0.0155\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0156 - val_loss: 0.0155\n",
      "Epoch 30/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0156Epoch 30/100 - loss: 0.0156 - val_loss: 0.0155\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0156 - val_loss: 0.0155\n",
      "Epoch 31/100\n",
      "133/138 [===========================>..] - ETA: 0s - loss: 0.0156Epoch 31/100 - loss: 0.0156 - val_loss: 0.0155\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0156 - val_loss: 0.0155\n",
      "Epoch 32/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0154Epoch 32/100 - loss: 0.0154 - val_loss: 0.0155\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0154 - val_loss: 0.0155\n",
      "Epoch 33/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0155Epoch 33/100 - loss: 0.0155 - val_loss: 0.0154\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 34/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0155Epoch 34/100 - loss: 0.0155 - val_loss: 0.0153\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0155 - val_loss: 0.0153\n",
      "Epoch 35/100\n",
      "137/138 [============================>.] - ETA: 0s - loss: 0.0153Epoch 35/100 - loss: 0.0153 - val_loss: 0.0151\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0153 - val_loss: 0.0151\n",
      "Epoch 36/100\n",
      "135/138 [============================>.] - ETA: 0s - loss: 0.0153Epoch 36/100 - loss: 0.0153 - val_loss: 0.0153\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0153 - val_loss: 0.0153\n",
      "Epoch 37/100\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0154Epoch 37/100 - loss: 0.0154 - val_loss: 0.0156\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0154 - val_loss: 0.0156\n",
      "Epoch 38/100\n",
      "138/138 [==============================] - ETA: 0s - loss: 0.0152Epoch 38/100 - loss: 0.0152 - val_loss: 0.0155\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0152 - val_loss: 0.0155\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/138 [============================>.] - ETA: 0s - loss: 0.0152Epoch 39/100 - loss: 0.0152 - val_loss: 0.0153\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0152 - val_loss: 0.0153\n",
      "Epoch 40/100\n",
      "134/138 [============================>.] - ETA: 0s - loss: 0.0152Epoch 40/100 - loss: 0.0152 - val_loss: 0.0154\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0152 - val_loss: 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/20 01:42:09 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpvak37k2o/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpvak37k2o/model/data/model/assets\n",
      "2024/08/20 01:42:13 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpvak37k2o/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/08/20 01:42:13 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n",
      "\n",
      "Huber Loss (both), fk=10e100,d01 Model:\n",
      "Mean Error: 0.0348\n",
      "Median Error: 0.0345\n",
      "90th Percentile Error: 0.0510\n",
      "Max Error: 4.1312\n",
      "Training Time: 53.59 seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        \"model_name\": \"Huber Loss (both), fk=5e100,d01\",\n",
    "        \"loss_type\": \"modified_custom\",\n",
    "        \"fk_weight\": 5,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": 65536,\n",
    "        \"initial_learning_rate\": 1e-3,\n",
    "        \"experiment_name\": \"Inverse Kinematics NN Comparison\"\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"Huber Loss (both), fk=10e100,d01\",\n",
    "        \"loss_type\": \"modified_custom\",\n",
    "        \"fk_weight\": 10,\n",
    "        \"epochs\": epochs,\n",
    "        \"initial_learning_rate\": 1e-3,\n",
    "        \"batch_size\": 65536,\n",
    "        \"experiment_name\": \"Inverse Kinematics NN Comparison\"\n",
    "    }\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for config in configs:\n",
    "    results[config['model_name']] = run_single_experiment(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': <keras.engine.sequential.Sequential at 0x7f86a9f25700>,\n",
       " 'history': <keras.callbacks.History at 0x7f8794c3ba00>,\n",
       " 'errors': array([0.05139498, 0.03106508, 0.02627196, ..., 0.02334953, 0.00690967,\n",
       "        0.01247718], dtype=float32),\n",
       " 'true_xyz': array([[ 0.6068389 , -1.5461596 ,  1.9428234 ],\n",
       "        [-1.2951391 ,  1.9623394 ,  1.2530879 ],\n",
       "        [-1.8338947 ,  1.0807508 ,  1.7556641 ],\n",
       "        ...,\n",
       "        [ 0.52119195, -1.9730337 , -0.3695886 ],\n",
       "        [ 0.19297831,  1.5389277 , -0.6839478 ],\n",
       "        [-1.3054239 ,  1.4633682 ,  0.14486456]], dtype=float32),\n",
       " 'predicted_xyz': array([[ 0.6283296 , -1.58539   ,  1.968133  ],\n",
       "        [-1.3041798 ,  1.9742658 ,  1.2803104 ],\n",
       "        [-1.8534274 ,  1.092737  ,  1.7685102 ],\n",
       "        ...,\n",
       "        [ 0.5131469 , -1.9599676 , -0.3871885 ],\n",
       "        [ 0.18861663,  1.5441757 , -0.6828629 ],\n",
       "        [-1.3052652 ,  1.4727107 ,  0.15313327]], dtype=float32),\n",
       " 'training_time': 100.56743788719177}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[configs[0]['model_name']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
