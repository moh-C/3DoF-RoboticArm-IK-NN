{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward kinematics function (for generating data and evaluation)\n",
    "def forward_kinematics_3dof(theta1, theta2, theta3, l1, l2, l3):\n",
    "    x = l1 * np.cos(theta1) * np.sin(theta2) + l2 * np.cos(theta1) * np.sin(theta2 + theta3)\n",
    "    y = l1 * np.sin(theta1) * np.sin(theta2) + l2 * np.sin(theta1) * np.sin(theta2 + theta3)\n",
    "    z = l1 * np.cos(theta2) + l2 * np.cos(theta2 + theta3) + l3\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset for inverse kinematics\n",
    "def generate_dataset(num_samples=100_000):\n",
    "    l1, l2, l3 = 1.0, 1.5, 0.5  # link lengths\n",
    "    theta1 = np.random.uniform(-np.pi/2, np.pi/2, num_samples)\n",
    "    theta2 = np.random.uniform(-np.pi/2, np.pi/2, num_samples)\n",
    "    theta3 = np.random.uniform(-np.pi/2, np.pi/2, num_samples)\n",
    "    \n",
    "    inputs = np.array([forward_kinematics_3dof(t1, t2, t3, l1, l2, l3) for t1, t2, t3 in zip(theta1, theta2, theta3)])\n",
    "    outputs = np.column_stack((theta1, theta2, theta3))\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(inputs, outputs, filename='robot_arm_dataset'):\n",
    "    np.savez(filename, inputs=inputs, outputs=outputs)\n",
    "    print(f\"Dataset saved to {filename}.npz\")\n",
    "\n",
    "def load_dataset(filename='robot_arm_dataset.npz'):\n",
    "    data = np.load(filename)\n",
    "    return data['inputs'], data['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model for Inverse Kinematics\n",
    "def create_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, activation='relu', input_shape=(3,)),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(3)  # Output: 3 joint angles\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Custom Learning Rate Schedule\n",
    "class CosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, peak_learning_rate, warmup_steps, decay_steps):\n",
    "        super(CosineDecayWithWarmup, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.peak_learning_rate = peak_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "        self.cosine_decay = tf.keras.experimental.CosineDecay(\n",
    "            peak_learning_rate, decay_steps - warmup_steps, alpha=initial_learning_rate / peak_learning_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        \n",
    "        warmup_lr = self.initial_learning_rate + (self.peak_learning_rate - self.initial_learning_rate) * (step / self.warmup_steps)\n",
    "        cosine_lr = self.cosine_decay(step - self.warmup_steps)\n",
    "        \n",
    "        lr = tf.cond(step < self.warmup_steps,\n",
    "                     lambda: warmup_lr,\n",
    "                     lambda: cosine_lr)\n",
    "        \n",
    "        return tf.cast(lr, tf.float32)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_learning_rate\": self.initial_learning_rate,\n",
    "            \"peak_learning_rate\": self.peak_learning_rate,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"decay_steps\": self.decay_steps,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(LearningRateLogger, self).__init__()\n",
    "        self.lr_rates = []\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, '__call__'):\n",
    "            lr = lr(self.model.optimizer.iterations)\n",
    "        self.lr_rates.append(lr.numpy())\n",
    "\n",
    "def train_network(model, train_dataset, val_dataset, epochs=100):\n",
    "    total_steps = epochs * len(train_dataset)\n",
    "    warmup_steps = total_steps // 10\n",
    "    \n",
    "    lr_schedule = CosineDecayWithWarmup(\n",
    "        initial_learning_rate=1e-4,\n",
    "        peak_learning_rate=1e-3,\n",
    "        warmup_steps=warmup_steps,\n",
    "        decay_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    lr_logger = LearningRateLogger()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True),\n",
    "            tf.keras.callbacks.CSVLogger('training_log.csv'),\n",
    "            early_stopping,\n",
    "            tf.keras.callbacks.TerminateOnNaN(),\n",
    "            lr_logger\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Save learning rates\n",
    "    np.save('learning_rates.npy', np.array(lr_logger.lr_rates))\n",
    "    \n",
    "    return model, history, lr_logger.lr_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Main Pipeline Execution\n",
    "# Generate dataset\n",
    "# print(\"Generating dataset...\")\n",
    "# inputs, outputs = generate_dataset()\n",
    "\n",
    "# # Save dataset\n",
    "# print(\"Saving dataset...\")\n",
    "# save_dataset(inputs, outputs)\n",
    "\n",
    "# Load dataset\n",
    "# print(\"Loading dataset...\")\n",
    "# inputs, outputs = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs[:5], outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the inputs and outputs\n",
    "# input_mean, input_std = np.mean(inputs, axis=0), np.std(inputs, axis=0)\n",
    "# output_mean, output_std = np.mean(outputs, axis=0), np.std(outputs, axis=0)\n",
    "\n",
    "# inputs_normalized = (inputs - input_mean) / (input_std + 1e-8)\n",
    "# outputs_normalized = (outputs - output_mean) / (output_std + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare datasets with normalized data\n",
    "# train_size = int(0.8 * len(inputs_normalized))\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "#     (inputs_normalized[:train_size], outputs_normalized[:train_size])\n",
    "# ).shuffle(buffer_size=10000).batch(1024).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "#     (inputs_normalized[train_size:], outputs_normalized[train_size:])\n",
    "# ).batch(1024).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# # Create and train model\n",
    "# print(\"Training neural network...\")\n",
    "# model = create_model()\n",
    "# model, history, lr_rates = train_network(model, train_dataset, val_dataset, epochs=50)\n",
    "\n",
    "# print(\"Training complete!\")\n",
    "\n",
    "# # Don't forget to save the normalization parameters for later use\n",
    "# np.savez('normalization_params.npz', \n",
    "#          input_mean=input_mean, input_std=input_std, \n",
    "#          output_mean=output_mean, output_std=output_std)\n",
    "\n",
    "# Main execution\n",
    "inputs, outputs = generate_dataset()\n",
    "train_size = int(0.8 * len(inputs))\n",
    "train_inputs, train_outputs = inputs[:train_size], outputs[:train_size]\n",
    "val_inputs, val_outputs = inputs[train_size:], outputs[train_size:]\n",
    "\n",
    "# Normalize data\n",
    "input_mean, input_std = np.mean(train_inputs, axis=0), np.std(train_inputs, axis=0)\n",
    "output_mean, output_std = np.mean(train_outputs, axis=0), np.std(train_outputs, axis=0)\n",
    "\n",
    "train_inputs_norm = (train_inputs - input_mean) / (input_std + 1e-8)\n",
    "train_outputs_norm = (train_outputs - output_mean) / (output_std + 1e-8)\n",
    "val_inputs_norm = (val_inputs - input_mean) / (input_std + 1e-8)\n",
    "val_outputs_norm = (val_outputs - output_mean) / (output_std + 1e-8)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs_norm, train_outputs_norm)).shuffle(10000).batch(1024)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_inputs_norm, val_outputs_norm)).batch(1024)\n",
    "\n",
    "model = create_model()\n",
    "trained_model, history, lr_rates = train_network(model, train_dataset, val_dataset, epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting cell\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lr_rates, label='Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('best_model.h5', custom_objects={'CosineDecayWithWarmup': CosineDecayWithWarmup})\n",
    "\n",
    "# Load the normalization parameters\n",
    "norm_params = np.load('normalization_params.npz')\n",
    "input_mean, input_std = norm_params['input_mean'], norm_params['input_std']\n",
    "output_mean, output_std = norm_params['output_mean'], norm_params['output_std']\n",
    "\n",
    "# Load the test dataset (assuming you've split your data into train and test)\n",
    "# If not, you can use a portion of your original dataset for testing\n",
    "inputs, outputs = load_dataset()  # Your function to load the dataset\n",
    "test_size = int(0.2 * len(inputs))  # Using 20% of data for testing\n",
    "test_inputs, test_outputs = inputs[-test_size:], outputs[-test_size:]\n",
    "\n",
    "# Normalize the test inputs\n",
    "test_inputs_normalized = (test_inputs - input_mean) / (input_std + 1e-8)\n",
    "\n",
    "# Make predictions\n",
    "predictions_normalized = model.predict(test_inputs_normalized)\n",
    "\n",
    "# Denormalize the predictions\n",
    "predictions = predictions_normalized * (output_std + 1e-8) + output_mean\n",
    "\n",
    "# Calculate Mean Absolute Error for each coordinate\n",
    "mae_x = mean_absolute_error(test_outputs[:, 0], predictions[:, 0])\n",
    "mae_y = mean_absolute_error(test_outputs[:, 1], predictions[:, 1])\n",
    "mae_z = mean_absolute_error(test_outputs[:, 2], predictions[:, 2])\n",
    "\n",
    "print(f\"Mean Absolute Error:\")\n",
    "print(f\"X: {mae_x:.4f}\")\n",
    "print(f\"Y: {mae_y:.4f}\")\n",
    "print(f\"Z: {mae_z:.4f}\")\n",
    "\n",
    "# Visualize the differences\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, coord in enumerate(['X', 'Y', 'Z']):\n",
    "    axes[i].scatter(test_outputs[:, i], predictions[:, i], alpha=0.1)\n",
    "    axes[i].plot([test_outputs[:, i].min(), test_outputs[:, i].max()], \n",
    "                 [test_outputs[:, i].min(), test_outputs[:, i].max()], \n",
    "                 'r--', lw=2)\n",
    "    axes[i].set_xlabel(f'Actual {coord}')\n",
    "    axes[i].set_ylabel(f'Predicted {coord}')\n",
    "    axes[i].set_title(f'{coord} Coordinate: Actual vs Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize error distribution\n",
    "error = predictions - test_outputs\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, coord in enumerate(['X', 'Y', 'Z']):\n",
    "    axes[i].hist(error[:, i], bins=50, edgecolor='black')\n",
    "    axes[i].set_xlabel(f'Error in {coord}')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'Error Distribution for {coord} Coordinate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3D scatter plot of actual vs predicted points\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(test_outputs[:, 0], test_outputs[:, 1], test_outputs[:, 2], c='blue', label='Actual', alpha=0.1)\n",
    "ax.scatter(predictions[:, 0], predictions[:, 1], predictions[:, 2], c='red', label='Predicted', alpha=0.1)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.legend()\n",
    "ax.set_title('Actual vs Predicted 3D Positions')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_inverse_kinematics(model, test_inputs, test_outputs, input_mean, input_std, output_mean, output_std, l1=1.0, l2=1.5, l3=0.5):\n",
    "    # Normalize test inputs\n",
    "    test_inputs_norm = (test_inputs - input_mean) / (input_std + 1e-8)\n",
    "    \n",
    "    # Get predictions and denormalize\n",
    "    predictions_norm = model.predict(test_inputs_norm)\n",
    "    predictions = predictions_norm * (output_std + 1e-8) + output_mean\n",
    "    \n",
    "    # Calculate angular error\n",
    "    angular_error = np.abs(test_outputs - predictions)\n",
    "    mean_angular_error = np.mean(angular_error, axis=0)\n",
    "    \n",
    "    # Calculate end-effector position using forward kinematics on predicted angles\n",
    "    predicted_positions = np.array([forward_kinematics_3dof(t1, t2, t3, l1, l2, l3) for t1, t2, t3 in predictions])\n",
    "    \n",
    "    # Calculate position error\n",
    "    position_error = np.abs(test_inputs - predicted_positions)\n",
    "    mean_position_error = np.mean(position_error, axis=0)\n",
    "    \n",
    "    print(\"Mean Angular Error (radians):\")\n",
    "    print(f\"Theta1: {mean_angular_error[0]:.4f}\")\n",
    "    print(f\"Theta2: {mean_angular_error[1]:.4f}\")\n",
    "    print(f\"Theta3: {mean_angular_error[2]:.4f}\")\n",
    "    \n",
    "    print(\"\\nMean Position Error (meters):\")\n",
    "    print(f\"X: {mean_position_error[0]:.4f}\")\n",
    "    print(f\"Y: {mean_position_error[1]:.4f}\")\n",
    "    print(f\"Z: {mean_position_error[2]:.4f}\")\n",
    "    \n",
    "    # Calculate relative position error\n",
    "    relative_error = position_error / (np.abs(test_inputs) + 1e-8)\n",
    "    mean_relative_error = np.mean(relative_error, axis=0)\n",
    "    \n",
    "    print(\"\\nMean Relative Position Error (%):\")\n",
    "    print(f\"X: {mean_relative_error[0]*100:.2f}%\")\n",
    "    print(f\"Y: {mean_relative_error[1]*100:.2f}%\")\n",
    "    print(f\"Z: {mean_relative_error[2]*100:.2f}%\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    for i, angle in enumerate(['Theta1', 'Theta2', 'Theta3']):\n",
    "        axes[0, i].scatter(test_outputs[:, i], predictions[:, i], alpha=0.1)\n",
    "        axes[0, i].plot([-np.pi/2, np.pi/2], [-np.pi/2, np.pi/2], 'r--')\n",
    "        axes[0, i].set_xlabel(f'Actual {angle} (radians)')\n",
    "        axes[0, i].set_ylabel(f'Predicted {angle} (radians)')\n",
    "        axes[0, i].set_title(f'{angle}: Actual vs Predicted')\n",
    "    \n",
    "    for i, coord in enumerate(['X', 'Y', 'Z']):\n",
    "        axes[1, i].scatter(test_inputs[:, i], predicted_positions[:, i], alpha=0.1)\n",
    "        axes[1, i].plot([test_inputs[:, i].min(), test_inputs[:, i].max()], \n",
    "                        [test_inputs[:, i].min(), test_inputs[:, i].max()], 'r--')\n",
    "        axes[1, i].set_xlabel(f'Original {coord} (meters)')\n",
    "        axes[1, i].set_ylabel(f'FK of Predicted {coord} (meters)')\n",
    "        axes[1, i].set_title(f'{coord}: Original vs FK of Predicted')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Histogram of position errors\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    for i, coord in enumerate(['X', 'Y', 'Z']):\n",
    "        axes[i].hist(position_error[:, i], bins=50, edgecolor='black')\n",
    "        axes[i].set_xlabel(f'{coord} Error (meters)')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].set_title(f'Distribution of {coord} Error')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# test_inputs, test_outputs = val_inputs, val_outputs  # Using validation set for testing\n",
    "# evaluate_inverse_kinematics(trained_model, (test_inputs - input_mean) / (input_std + 1e-8), test_outputs)\n",
    "\n",
    "evaluate_inverse_kinematics(model, test_inputs, test_outputs, input_mean, input_std, output_mean, output_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
